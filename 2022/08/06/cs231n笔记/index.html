<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    <meta name="description" content="Hexo Theme Redefine">
    <meta name="author" content="staaar">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-redefine.png">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preconnect" href="https://evan.beee.top" crossorigin>
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2022/08/06/cs231n笔记/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="cs231nBUG好像有点多, 罢了, 反正也不是我看, 嘻嘻 卷积神经网络 -&gt; 图像 循环神经网络 -&gt; 语言 正则化 -&gt; 减少模型复杂度 损失函数 -&gt; 衡量分类算法的预测结果和真实标签结果之间的一致性 Nearest Neighbor分类器L1距离$$d_1(I_1,I_2) &#x3D; \sum\limits_{p}\lvert I_1^P - I_2^P \">
<meta property="og:type" content="article">
<meta property="og:title" content="cs231n笔记">
<meta property="og:url" content="http://example.com/2022/08/06/cs231n%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="cs231nBUG好像有点多, 罢了, 反正也不是我看, 嘻嘻 卷积神经网络 -&gt; 图像 循环神经网络 -&gt; 语言 正则化 -&gt; 减少模型复杂度 损失函数 -&gt; 衡量分类算法的预测结果和真实标签结果之间的一致性 Nearest Neighbor分类器L1距离$$d_1(I_1,I_2) &#x3D; \sum\limits_{p}\lvert I_1^P - I_2^P \">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_i=0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda%3E1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda+W">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=N">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_j">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=R(W)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f(x_i;W)=Wx_i">
<meta property="og:image" content="https://pic1.zhimg.com/a90ce9e0ff533f3efee4747305382064_r.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5B1,-2,0%5D%5Cto%5Be%5E1,e%5E%7B-2%7D,e%5E0%5D=%5B2.71,0.14,1%5D%5Cto%5B0.7,0.04,0.26%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5B0.5,-1,0%5D%5Cto%5Be%5E%7B0.5%7D,e%5E%7B-1%7D,e%5E0%5D=%5B1.65,0.73,1%5D%5Cto%5B0.55,0.12,0.33%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdelta+W">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W+%5Cdelta+W">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w_%7By_i%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=j%5Cnot+=y_i">
<meta property="og:image" content="https://pic2.zhimg.com/03b3eccf18ee3760e219f9f95ec14305_r.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%7D%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+x%7D">
<meta property="og:image" content="https://pic1.zhimg.com/0799b3d6e5e92245ee937db3c26d1b80_r.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=max(0,-)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_1,W_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma(x)=1/(1+e%5E%7B-x%7D)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Csigma(%5CSigma_iw_ix_i+b)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=P(y_i=1%7Cx_i;w)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=P(y_i=0%7Cx_i;w)=1-P(y_i=1%7Cx_i;w)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w">
<meta property="og:image" content="https://pic3.zhimg.com/80/677187e96671a4cac9c95352743b3806_720w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f=w%5ETx+b">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x%3E0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f">
<meta property="og:image" content="https://pic3.zhimg.com/80/677187e96671a4cac9c95352743b3806_720w.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/83682a138f6224230f5b0292d9c01bd2_720w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x=0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x%3E0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f(x)=1(x%3C0)(%5Calpha+x)+1(x%3E=0)(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f(w%5ETx+b)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=max(w%5ET_1x+b_1,w%5ET_2x+b_2)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w_1,b_1=0">
<meta property="og:image" content="https://pic3.zhimg.com/80/ccb56c1fb267bc632d6d88459eb14ace_720w.png">
<meta property="og:image" content="https://pic1.zhimg.com/80/e743b6777775b1671c3b5503d7afbbc4_720w.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/aae11de6e6a29f50d46b9ea106fbb02a_720w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5Clambda+w%5E2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda+w">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=2%5Clambda+w">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda%7Cw%7C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda_1%7Cw%7C+%5Clambda_2w%5E2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Coverrightarrow%7Bw%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%7C%7C%5Coverrightarrow%7Bw%7D%7C%7C_2%3Cc">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://pic4.zhimg.com/80/63fcf4cc655cb04f21a37e86aca333cf_720w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=j">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_%7Bij%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_j">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+P(y=0%7Cx;w,b)=1-P(y=1%7Cx;w,b)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma(w%5ETx+b)%3E0.5">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w%5ETx+b%3E0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_%7Bij%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma(.)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+f_j%7D=y_%7Bij%7D-%5Csigma(f_j)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csum_j">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cpartial+L_i/%5Cpartial+f_j">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=sign(%5Cdelta_%7Bij%7D)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=max(0,x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x=-1e6">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x%3C0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f(x+h)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h%3E1e-6">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=max(x,y)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f(x+h)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f(x-h)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f(x+h)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f(x-h)">
<meta property="og:image" content="https://pic2.zhimg.com/80/753f398b46cc28c1916d6703cf2080f5_720w.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/05a6960a01c0204ced8d875ac3d91fba_720w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/96573094f9d7f4b3b188069726840a2e_720w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=U=mgh">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=U%5Cpropto+h">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F=-%5Cnabla+U">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F=ma">
<meta property="og:image" content="https://pic1.zhimg.com/80/412afb713ddcff0ba9165ab026563304_720w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha=%5Calpha_0e%5E%7B-kt%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha_0,k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha=%5Calpha_0/(1+kt)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha_0,k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Hf(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cnabla+f(x)">
<meta property="og:image" content="https://pic4.zhimg.com/80/d25cf561835c7b96ae6d1c91868bcbff_720w.png">
<meta property="og:image" content="https://pic2.zhimg.com/80/2ef08bb4cf60805d726b2d6db39dd985_720w.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/641c8846abcb02d35938660cf96cef1b_720w.jpg">
<meta property="og:image" content="c:/Users/admin/AppData/Roaming/Typora/typora-user-images/image-20220728141015233.png">
<meta property="og:image" content="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/10.png">
<meta property="og:image" content="c:/Users/admin/AppData/Roaming/Typora/typora-user-images/image-20220728134844261.png">
<meta property="og:image" content="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/11.png">
<meta property="og:image" content="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/12.png">
<meta property="og:image" content="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/13.png">
<meta property="og:image" content="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/14.png">
<meta property="og:image" content="c:/Users/admin/AppData/Roaming/Typora/typora-user-images/image-20220728151108816.png">
<meta property="og:image" content="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/18.png">
<meta property="og:image" content="c:/Users/admin/AppData/Roaming/Typora/typora-user-images/image-20220728154007052.png">
<meta property="og:image" content="c:/Users/admin/AppData/Roaming/Typora/typora-user-images/image-20220728191010720.png">
<meta property="og:image" content="c:/Users/admin/AppData/Roaming/Typora/typora-user-images/image-20220728190941669.png">
<meta property="og:image" content="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/20.png">
<meta property="og:image" content="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/48.png">
<meta property="og:image" content="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/51.png">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-ab028874d83229781b5c8cd9f3a9d6f4_720w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-544b089740395a57c21fa585d573702e_720w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i%5Cin%5C%7B-1,1%5C%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C%5Cpropto%5Cfrac%7B1%7D%7B%5Clambda%7D">
<meta property="og:image" content="https://pic1.zhimg.com/80/cf3fc543bf1dc81e2083530a4492b0ec_720w.png">
<meta property="og:image" content="https://images0.cnblogs.com/blog2015/680781/201508/082141550183343.jpg">
<meta property="og:image" content="https://images0.cnblogs.com/blog2015/680781/201508/082142031439693.jpg">
<meta property="og:image" content="https://images0.cnblogs.com/blog2015/680781/201508/082142437997206.jpg">
<meta property="og:image" content="https://images0.cnblogs.com/blog2015/680781/201508/082143466598401.jpg">
<meta property="og:image" content="https://images0.cnblogs.com/blog2015/680781/201508/082145081436842.jpg">
<meta property="og:image" content="https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG">
<meta property="article:published_time" content="2022-08-05T16:00:00.000Z">
<meta property="article:modified_time" content="2023-03-30T13:01:36.697Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=L_i=0">
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/star.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/star.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/star.svg">
    <!--- Page Info-->
    
    <title>
        
            cs231n笔记 -
        
        Star
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/assets/fonts.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"example.com","root":"/","language":"zh-CN"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center"},"word_count":{"enable":false,"count":false,"min2read":false},"author_label":{"enable":false,"auto":false,"list":[""]},"code_block":{"copy":true,"style":"simple","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":false,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":false,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true},"home_banner":{"enable":true,"image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"一条咸鱼罢了","subtitle":{"text":[],"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"audios":[{"name":null,"artist":null,"url":null,"cover":null},{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.1.0","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Categories":{"icon":"fa-solid fa-folder","path":"/categories/"},"Archives":{"icon":"fa-regular fa-archive","path":"/archives/"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","announcement":null,"links":{"Categories":{"path":"/categories","icon":"fa-regular fa-folder"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"}}},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}}};
    Global.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 5.4.2"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Star
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        首页
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/categories/"  >
                                    
                                        
                                            <i class="fa-solid fa-folder"></i>
                                        
                                        分类
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives/"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        归档
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                首页
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/categories/"  >
                             
                                
                                    <i class="fa-solid fa-folder"></i>
                                
                                分类
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/archives/"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                归档
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">cs231n笔记</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/cat.jpg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">staaar</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2022-08-06</span>
        <span class="mobile">2022-08-06 00</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-03-30 21:01:36</span>
            <span class="mobile">2023-03-30 21:01</span>
            <span class="hover-info">更新</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    

    
    
    
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="cs231n"><a href="#cs231n" class="headerlink" title="cs231n"></a>cs231n</h1><p>BUG好像有点多, 罢了, 反正也不是我看, 嘻嘻</p>
<p>卷积神经网络 -&gt; 图像</p>
<p>循环神经网络 -&gt; 语言</p>
<p>正则化 -&gt; 减少模型复杂度</p>
<p>损失函数 -&gt; 衡量分类算法的预测结果和真实标签结果之间的一致性</p>
<h2 id="Nearest-Neighbor分类器"><a href="#Nearest-Neighbor分类器" class="headerlink" title="Nearest Neighbor分类器"></a>Nearest Neighbor分类器</h2><h5 id="L1距离"><a href="#L1距离" class="headerlink" title="L1距离"></a>L1距离</h5><p>$$<br>d_1(I_1,I_2) &#x3D; \sum\limits_{p}\lvert I_1^P - I_2^P \rvert<br>$$</p>
<h5 id="L2距离"><a href="#L2距离" class="headerlink" title="L2距离"></a>L2距离</h5><p>也称欧几里得距离，是最常见的距离度量，衡量的是多维空间中两个点之间的<strong>绝对距离</strong> 。<br>$$<br>d &#x3D; \sqrt{ \sum\limits_{i&#x3D;1}^{n}(x_i-y_i)^2}<br>$$</p>
<p><strong>L1和L2比较</strong>。比较这两个度量方式是挺有意思的。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在<a class="link"   href="https://link.zhihu.com/?target=http://planetmath.org/vectorpnorm" >p-norm <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>常用的特殊形式。</p>
<h5 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h5><p><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45626815/article/details/109188186" >https://blog.csdn.net/weixin_45626815/article/details/109188186 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>缺点：</p>
<ol>
<li>运行时间太长</li>
<li>像欧几里得距离或者L1距离这样的衡量标准用在比较图像上不太适合</li>
</ol>
<h2 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h2><h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>衡量我们对结果的不满意程度<br>$$<br>L_i &#x3D; \sum\limits_{j≠y_i} max(0, s_j - s_{y_i}+\Delta)<br>$$</p>
<h5 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h5><p>上面损失函数有一个问题。假设有一个数据集和一个权重集<strong>W</strong>能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=L_i=0"
                      alt="[公式]"
                >）。问题在于这个<strong>W</strong>并不唯一：可能有很多相似的<strong>W</strong>都能正确地分类所有的数据。一个简单的例子：如果<strong>W</strong>能够正确分类所有数据，即对于每个数据，损失值都是0。那么当<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Clambda%3E1"
                      alt="[公式]"
                >时，任何数乘<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Clambda+W"
                      alt="[公式]"
                >都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对<strong>W</strong>乘以2将使得差距变成30。</p>
<p><strong>目的</strong>：防止模型过拟合</p>
<p><strong>原理</strong>：在损失函数上加上某些规则（限制），缩小解空间，从而减少求出过拟合解的可能性</p>
<h5 id="正则化惩罚"><a href="#正则化惩罚" class="headerlink" title="正则化惩罚"></a>正则化惩罚</h5><p>$$<br>R(W) &#x3D; \sum\limits_k \sum\limits_l W^2_{k,l}<br>$$</p>
<h5 id="SVM损失函数"><a href="#SVM损失函数" class="headerlink" title="SVM损失函数"></a>SVM损失函数</h5><p>$$<br>L &#x3D; \underbrace{\frac{1}{N} \sum\limits_iL_i}_<br>{\rm data ,loss},+,\underbrace{\lambda R(W)}_{\rm regularization ,loss}<br>$$</p>
<p>$$<br>L&#x3D;\frac{1}{N}\sum_i\sum_{j≠{y_i}}[max(0,f(x_i;W)<em>j-f(x_i;W)</em>{y_i})+\Delta]+\lambda\sum_k\sum_lW^2_{k,l}<br>$$</p>
<p>其中，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=N"
                      alt="[公式]"
                >是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Clambda"
                      alt="[公式]"
                >来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。</p>
<p>只关注于正确的分数比不正确的分数大过某个数值 min0, max∞</p>
<p>计算梯度下降的grads:<br>$$<br>\begin{aligned}<br>\frac{dL_i}{dw_j} &amp; &#x3D;1(x_iw_j-x_iw_{y_i}+\Delta &gt;0)<br>\begin{bmatrix} x_{i1} \x_{i2} \ \vdots \x_{iD}<br>\end{bmatrix} \<br>&amp; &#x3D; 1(x_iw_j-x_iw_{y_i}+\Delta &gt;0)x_i<br>\end{aligned}<br>$$</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:, j] += X[i,:].T</span><br></pre></td></tr></table></figure></div>

<p>当j&#x3D;yi时<br>$$<br>\frac{d(x_iw_j)}{dw_{y_i}}&#x3D;0, ,,, \frac{d(-x_iw_{y_i})}{dw_{y_i}}&#x3D;-x_i<br>$$<br>因此<br>$$<br>\begin{aligned}<br>\frac{dL_i}{dw_j} &amp; &#x3D;-\sum_{j≠y_i}(x_iw_j-x_iw_{y_i}+\Delta &gt;0)<br>\begin{bmatrix} x_{i1} \x_{i2} \ \vdots \x_{iD}<br>\end{bmatrix} \<br>&amp; &#x3D; -\sum_{j≠y_i}(x_iw_j-x_iw_{y_i}+\Delta &gt;0)x_i<br>\end{aligned}<br>$$</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:, y[i]] += -X[i,:].T</span><br></pre></td></tr></table></figure></div>



<h5 id="无正则化部分python实现"><a href="#无正则化部分python实现" class="headerlink" title="无正则化部分python实现"></a>无正则化部分python实现</h5><h6 id="非向量"><a href="#非向量" class="headerlink" title="非向量"></a>非向量</h6><p>常规方式，将每个类别单独与正确类别相减+Δ，循环多次</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_i</span>(<span class="params">x, y, W</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  unvectorized version. Compute the multiclass svm loss for a single example (x,y)</span></span><br><span class="line"><span class="string">  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)</span></span><br><span class="line"><span class="string">    with an appended bias dimension in the 3073-rd position (i.e. bias trick)</span></span><br><span class="line"><span class="string">  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)</span></span><br><span class="line"><span class="string">  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  delta = <span class="number">1.0</span> <span class="comment"># see notes about delta later in this section</span></span><br><span class="line">  scores = W.dot(x) <span class="comment"># scores becomes of size 10 x 1, the scores for each class</span></span><br><span class="line">  correct_class_score = scores[y]</span><br><span class="line">  D = W.shape[<span class="number">0</span>] <span class="comment"># number of classes, e.g. 10</span></span><br><span class="line">  loss_i = <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> xrange(D): <span class="comment"># iterate over all wrong classes</span></span><br><span class="line">    <span class="keyword">if</span> j == y:</span><br><span class="line">      <span class="comment"># skip for the true class to only loop over incorrect classes</span></span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># accumulate loss for the i-th example</span></span><br><span class="line">    loss_i += <span class="built_in">max</span>(<span class="number">0</span>, scores[j] - correct_class_score + delta)</span><br><span class="line">  <span class="keyword">return</span> loss_i</span><br></pre></td></tr></table></figure></div>

<h6 id="半向量化"><a href="#半向量化" class="headerlink" title="半向量化"></a>半向量化</h6><p>将所有类别不同分数排成矩阵，再减去正确的那一行</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_i_vectorized</span>(<span class="params">x, y, W</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  A faster half-vectorized implementation. half-vectorized</span></span><br><span class="line"><span class="string">  refers to the fact that for a single example the implementation contains</span></span><br><span class="line"><span class="string">  no for loops, but there is still one loop over the examples (outside this function)</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  delta = <span class="number">1.0</span></span><br><span class="line">  scores = W.dot(x)</span><br><span class="line">  <span class="comment"># compute the margins for all classes in one vector operation</span></span><br><span class="line">  margins = np.maximum(<span class="number">0</span>, scores - scores[y] + delta)</span><br><span class="line">  <span class="comment"># on y-th position scores[y] - scores[y] canceled and gave delta. We want</span></span><br><span class="line">  <span class="comment"># to ignore the y-th position and only consider margin on max wrong class</span></span><br><span class="line">  margins[y] = <span class="number">0</span></span><br><span class="line">  loss_i = np.<span class="built_in">sum</span>(margins)</span><br><span class="line">  <span class="keyword">return</span> loss_i</span><br></pre></td></tr></table></figure></div>

<h6 id="全向量化"><a href="#全向量化" class="headerlink" title="全向量化"></a>全向量化</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L</span>(<span class="params">X, y, W</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  fully-vectorized implementation :</span></span><br><span class="line"><span class="string">  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)</span></span><br><span class="line"><span class="string">  - y is array of integers specifying correct class (e.g. 50,000-D array)</span></span><br><span class="line"><span class="string">  - W are weights (e.g. 10 x 3073)</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># evaluate loss over all examples in X without using any for loops</span></span><br><span class="line">  <span class="comment"># left as exercise to reader in the assignment</span></span><br></pre></td></tr></table></figure></div>



<h5 id="softmax分类器"><a href="#softmax分类器" class="headerlink" title="softmax分类器"></a>softmax分类器</h5><p>softmax函数<br>$$<br>f_j(z)&#x3D;\frac{e^{z_j}}{\sum_ke^{z_k}}<br>$$<br>使用<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f_j"
                      alt="[公式]"
                >来表示分类评分向量<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f"
                      alt="[公式]"
                >中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=L_i"
                      alt="[公式]"
                >的均值与正则化损失<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=R(W)"
                      alt="[公式]"
                >之和。</p>
<p>交叉熵损失<br>$$<br>L_i&#x3D;-log(\frac{e^{z_j}}{\sum_ke^{z_k}}),,,or,,, L_i&#x3D;-f_{y_i}+log(\sum_ke^{z_k})<br>$$</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通计算</span></span><br><span class="line">scores = X[i].dot(W)</span><br><span class="line">correct_class_score = scores[y[i]]</span><br><span class="line">exp_sum = np.<span class="built_in">sum</span>(np.exp(scores))</span><br><span class="line">loss += np.log(exp_sum) - correct_class_score</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向量计算</span></span><br><span class="line">scores = X.dot(W)</span><br><span class="line">correct_class_score = scores[np.arange(num_train),y].reshape(num_train, <span class="number">1</span>)</span><br><span class="line">exp_sum = np.<span class="built_in">sum</span>(np.exp(scores), axis=<span class="number">1</span>).reshape(num_train, <span class="number">1</span>)</span><br><span class="line">loss += np.<span class="built_in">sum</span>(np.log(exp_sum) - correct_class_score)</span><br></pre></td></tr></table></figure></div>

<p>求梯度需要对交叉熵损失函数对正确分类W与错误分类W分别求偏导，具体公式如下，<br>$$<br>\nabla <em>{w</em>{yi}}L_i&#x3D;(-1+\frac{e^{f_{y_i}}}{\sum_je^{S_j}})X[i] \<br>\nabla _{w_j}L_i&#x3D;\frac{e^{f_j}}{\sum_je^j}X[i]<br>$$</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先将-X[i]放入矩阵, 避免判断情况</span></span><br><span class="line">dW[:, y[i]] += - X[i]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">	dW[:, j] += (np.exp(scores[j]) / exp_sum) * X[i]</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 比较容易理解的方式</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</span><br><span class="line">	<span class="keyword">if</span> j == y[i]</span><br><span class="line">		dW[:, j] += (np.exp(scores[j]) / exp_sum) * X[i] - X[i]</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		dW[:, j] += (np.exp(scores[j]) / exp_sum) * X[i]</span><br></pre></td></tr></table></figure></div>

<p>对于某个样本，其对正确分类的贡献比错误分类的贡献小 xi ，计算好右边那一大堆后直接对正确分类减1就行了：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向量方式</span></span><br><span class="line">margin = np.exp(scores) / exp_sum</span><br><span class="line">margin[np.arange(num_train), y] += -<span class="number">1</span></span><br><span class="line">dW = X.T.dot(margin)</span><br></pre></td></tr></table></figure></div>

<p>在Softmax分类器中，函数映射<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f(x_i;W)=Wx_i"
                      alt="[公式]"
                >保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge loss）</em>替换为<strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong>。</p>
<p>softmax方法的一个优势在于它可以让你的模型输出结果多样化，比如你的模型可能有相同的输入或者相同的前缀，或者图像标注时使用相同的图像，但是如果你使用概率分布而不是选择得分最大的，那么你会发现这些训练模型实际上可以产生多组不同类型的合理的输出序列，这取决于第一时步中输入的样本。</p>
<h5 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h5><p>SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。</p>
<p>Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic1.zhimg.com/a90ce9e0ff533f3efee4747305382064_r.jpg"
                      alt="preview"
                ></p>
<p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p>
<p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5B1,-2,0%5D%5Cto%5Be%5E1,e%5E%7B-2%7D,e%5E0%5D=%5B2.71,0.14,1%5D%5Cto%5B0.7,0.04,0.26%5D"
                      alt="[公式]"
                ></p>
<p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5B0.5,-1,0%5D%5Cto%5Be%5E%7B0.5%7D,e%5E%7B-1%7D,e%5E0%5D=%5B1.65,0.73,1%5D%5Cto%5B0.55,0.12,0.33%5D"
                      alt="[公式]"
                ></p>
<p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较 得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p>
<h2 id="最优化Optimization"><a href="#最优化Optimization" class="headerlink" title="最优化Optimization"></a>最优化Optimization</h2><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><h5 id="随机搜索"><a href="#随机搜索" class="headerlink" title="随机搜索"></a>随机搜索</h5><p>随机尝试很多不同的权重，然后看其中哪个最好。（差劲）</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设X_train的每一列都是一个数据样本（比如3073 x 50000）</span></span><br><span class="line"><span class="comment"># 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）</span></span><br><span class="line"><span class="comment"># 假设函数L对损失函数进行评价</span></span><br><span class="line"></span><br><span class="line">bestloss = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="comment"># Python assigns the highest possible float value</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.0001</span> <span class="comment"># generate random parameters</span></span><br><span class="line">  loss = L(X_train, Y_train, W) <span class="comment"># get the loss over the entire training set</span></span><br><span class="line">  <span class="keyword">if</span> loss &lt; bestloss: <span class="comment"># keep track of the best solution</span></span><br><span class="line">    bestloss = loss</span><br><span class="line">    bestW = W</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;in attempt %d the loss was %f, best %f&#x27;</span> % (num, loss, bestloss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># in attempt 0 the loss was 9.401632, best 9.401632</span></span><br><span class="line"><span class="comment"># in attempt 1 the loss was 8.959668, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 2 the loss was 9.044034, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 3 the loss was 9.278948, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 4 the loss was 8.857370, best 8.857370</span></span><br><span class="line"><span class="comment"># in attempt 5 the loss was 8.943151, best 8.857370</span></span><br><span class="line"><span class="comment"># in attempt 6 the loss was 8.605604, best 8.605604</span></span><br><span class="line"><span class="comment"># ... (trunctated: continues for 1000 lines)</span></span><br></pre></td></tr></table></figure></div>

<h5 id="随机本地搜索"><a href="#随机本地搜索" class="headerlink" title="随机本地搜索"></a>随机本地搜索</h5><p>第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=W"
                      alt="[公式]"
                >开始，然后生成一个随机的扰动<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cdelta+W"
                      alt="[公式]"
                > ，只有当<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=W+%5Cdelta+W"
                      alt="[公式]"
                >的损失值变低，我们才会更新。这个过程的具体代码如下：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 生成随机初始W</span></span><br><span class="line">bestloss = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  step_size = <span class="number">0.0001</span></span><br><span class="line">  Wtry = W + np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * step_size</span><br><span class="line">  loss = L(Xtr_cols, Ytr, Wtry)</span><br><span class="line">  <span class="keyword">if</span> loss &lt; bestloss:</span><br><span class="line">    W = Wtry</span><br><span class="line">    bestloss = loss</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;iter %d loss is %f&#x27;</span> % (i, bestloss)</span><br></pre></td></tr></table></figure></div>

<h5 id="跟随梯度"><a href="#跟随梯度" class="headerlink" title="跟随梯度"></a>跟随梯度</h5><p>在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数derivatives）。对一维函数的求导公式如下：<br>$$<br>\frac{df(x)}{dx} &#x3D; \lim_{h→0}\frac{f(x+h)-f(x)}{h}<br>$$</p>
<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><h5 id="数值计算梯度"><a href="#数值计算梯度" class="headerlink" title="数值计算梯度"></a>数值计算梯度</h5><p>公式为跟随梯度公式</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">eval_numerical_gradient</span>(<span class="params">f, x</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;  </span></span><br><span class="line"><span class="string">  一个f在x处的数值梯度法的简单实现</span></span><br><span class="line"><span class="string">  - f是只有一个参数的函数</span></span><br><span class="line"><span class="string">  - x是计算梯度的点</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line">  fx = f(x) <span class="comment"># 在原点计算函数值</span></span><br><span class="line">  grad = np.zeros(x.shape)</span><br><span class="line">  h = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 对x中所有的索引进行迭代</span></span><br><span class="line">  it = np.nditer(x, flags=[<span class="string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="string">&#x27;readwrite&#x27;</span>])</span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算x+h处的函数值</span></span><br><span class="line">    ix = it.multi_index</span><br><span class="line">    old_value = x[ix]</span><br><span class="line">    x[ix] = old_value + h <span class="comment"># 增加h</span></span><br><span class="line">    fxh = f(x) <span class="comment"># 计算f(x + h)</span></span><br><span class="line">    x[ix] = old_value <span class="comment"># 存到前一个值中 (非常重要)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算偏导数</span></span><br><span class="line">    grad[ix] = (fxh - fx) / h <span class="comment"># 坡度</span></span><br><span class="line">    it.iternext() <span class="comment"># 到下个维度</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure></div>

<p>实际中用<strong>中心差值公式</strong>效果较好</p>
<p>可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10损失函数的梯度：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要使用上面的代码我们需要一个只有一个参数的函数</span></span><br><span class="line"><span class="comment"># (在这里参数就是权重)所以也包含了X_train和Y_train</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CIFAR10_loss_fun</span>(<span class="params">W</span>):</span><br><span class="line">  <span class="keyword">return</span> L(X_train, Y_train, W)</span><br><span class="line"></span><br><span class="line">W = np.random.rand(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 随机权重向量</span></span><br><span class="line">df = eval_numerical_gradient(CIFAR10_loss_fun, W) <span class="comment"># 得到梯度</span></span><br></pre></td></tr></table></figure></div>

<p>梯度告诉我们损失函数在每个维度上的斜率，以此来进行更新：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">loss_original = CIFAR10_loss_fun(W) <span class="comment"># 初始损失值</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;original loss: %f&#x27;</span> % (loss_original, )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同步长的效果</span></span><br><span class="line"><span class="keyword">for</span> step_size_log <span class="keyword">in</span> [-<span class="number">10</span>, -<span class="number">9</span>, -<span class="number">8</span>, -<span class="number">7</span>, -<span class="number">6</span>, -<span class="number">5</span>,-<span class="number">4</span>,-<span class="number">3</span>,-<span class="number">2</span>,-<span class="number">1</span>]:</span><br><span class="line">  step_size = <span class="number">10</span> ** step_size_log</span><br><span class="line">  W_new = W - step_size * df <span class="comment"># 权重空间中的新位置</span></span><br><span class="line">  loss_new = CIFAR10_loss_fun(W_new)</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;for step size %f new loss: %f&#x27;</span> % (step_size, loss_new)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># original loss: 2.200718</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-10 new loss: 2.200652</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-09 new loss: 2.200057</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-08 new loss: 2.194116</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-07 new loss: 2.135493</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-06 new loss: 1.647802</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-05 new loss: 2.844355</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-04 new loss: 25.558142</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-03 new loss: 254.086573</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-02 new loss: 2539.370888</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-01 new loss: 25392.214036</span></span><br></pre></td></tr></table></figure></div>

<p><strong>在梯度负方向上更新</strong>：在上面的代码中，为了计算<strong>W_new</strong>，要注意我们是向着梯度<strong>df</strong>的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。</p>
<p><strong>步长的影响</strong>：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。</p>
<p>学习率过小：情况可能比较稳定但是进展较慢。</p>
<p>学习率过大：在某些点如果步长过大，反而可能越过最低点导致更高的损失值。</p>
<p><strong>效率问题</strong>：计算数值梯度的复杂性和参数的量线性相关。不适合大规模数据。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 作业中学习率的处理</span></span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> rs <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        sm = Softmax()</span><br><span class="line">        loss_hist = sm.train(X_train, y_train, learning_rate=lr, reg=rs, num_iters=<span class="number">1500</span>, verbose=<span class="literal">True</span>)</span><br><span class="line">        y_train_pred = sm.predict(X_train)</span><br><span class="line">        acc_tr = np.mean(y_train == y_train_pred)</span><br><span class="line">        y_val_pred = sm.predict(X_val)</span><br><span class="line">        acc_val = np.mean(y_val == y_val_pred)</span><br><span class="line">        </span><br><span class="line">        results[(lr, rs)] = (acc_tr, acc_val)</span><br><span class="line">        <span class="keyword">if</span> acc_val &gt; best_val:</span><br><span class="line">            best_val = acc_val</span><br><span class="line">            best_softmax = sm</span><br></pre></td></tr></table></figure></div>



<h5 id="微分分析计算梯度"><a href="#微分分析计算梯度" class="headerlink" title="微分分析计算梯度"></a>微分分析计算梯度</h5><p>用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。</p>
<p>为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做<strong>梯度检查</strong>。</p>
<p>SVM的损失函数中对<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=w_%7By_i%7D"
                      alt="[公式]"
                >进行微分得到：<br>$$<br>\nabla_{w_{y_i}}L_i&#x3D;-(\sum_{j≠{y_i}}\mathbb I（w^T_jx_i-w^T_{y_i}x_i+\Delta&gt;0))x_i<br>$$<br>其中<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=1"
                      alt="[公式]"
                >是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=x_i"
                      alt="[公式]"
                >就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=j%5Cnot+=y_i"
                      alt="[公式]"
                >行的梯度是：<br>$$<br>\nabla_{w_j}L_i&#x3D;-(\sum_{j≠{y_i}}\mathbb I（w^T_jx_i-w^T_{y_i}x_i+\Delta&gt;0))x_i<br>$$</p>
<h6 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 进行梯度更新</span></span><br></pre></td></tr></table></figure></div>

<p>到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的小批量数据梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  data_batch = sample_training_data(data, <span class="number">256</span>) <span class="comment"># 256个数据</span></span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 参数更新</span></span><br></pre></td></tr></table></figure></div>

<p>在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。</p>
<h6 id="随机梯度下降SGD"><a href="#随机梯度下降SGD" class="headerlink" title="随机梯度下降SGD"></a>随机梯度下降SGD</h6><p>Stochastic Gradient Descent</p>
<p>小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本。</p>
<p>小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。</p>
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic2.zhimg.com/03b3eccf18ee3760e219f9f95ec14305_r.jpg"
                      alt="preview"
                ></p>
<p>信息流的总结图例。数据集中的(x,y)是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量<strong>f</strong>中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分f和实际标签y之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h5 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h5><p>反向传播是利用<strong>链式法则</strong>递归计算表达式的梯度的方法。</p>
<p>函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度。</p>
<p><strong>链式法则</strong>指出将这些梯度表达式链接起来的正确方式是相乘，比如<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%7D%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+x%7D"
                      alt="[公式]"
                >。在实际操作中，这只是简单地将两个梯度数值相乘，示例代码如下：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置输入值</span></span><br><span class="line">x = -<span class="number">2</span>; y = <span class="number">5</span>; z = -<span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">q = x + y <span class="comment"># q becomes 3</span></span><br><span class="line">f = q * z <span class="comment"># f becomes -12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行反向传播:</span></span><br><span class="line"><span class="comment"># 首先回传到 f = q * z</span></span><br><span class="line">dfdz = q <span class="comment"># df/dz = q, 所以关于z的梯度是3</span></span><br><span class="line">dfdq = z <span class="comment"># df/dq = z, 所以关于q的梯度是-4</span></span><br><span class="line"><span class="comment"># 现在回传到q = x + y</span></span><br><span class="line">dfdx = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dx = 1. 这里的乘法是因为链式法则</span></span><br><span class="line">dfdy = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dy = 1</span></span><br></pre></td></tr></table></figure></div>



<p>链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。</p>
<p>下面通过例子来对这一过程进行理解。加法门收到了输入[-2, 5]，计算输出是3。既然这个门是加法操作，那么对于两个输入的局部梯度都是+1。网络的其余部分计算出最终值为-12。在反向传播时将递归地使用链式法则，算到加法门（是乘法门的输入）的时候，知道加法门的输出的梯度是-4。如果网络如果想要输出值更高，那么可以认为它会想要加法门的输出更小一点（因为负号），而且还有一个4的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让-4乘以<strong>x</strong>和<strong>y</strong>的局部梯度，x和y的局部梯度都是1，所以最终都是-4）。可以看到得到了想要的效果：如果<strong>x，y减小</strong>（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。</p>
<p>因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是<strong>为了让整个网络的输出值更高</strong>。</p>
<h5 id="模块化"><a href="#模块化" class="headerlink" title="模块化"></a>模块化</h5><p>任何可微分的函数都可以看做门。可以将多个门组合成一个门，也可以根据需要将一个函数分拆成多个门。</p>
<h6 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h6><p>$$<br>\begin{array}<br>\sigma(x)&#x3D;\frac{1}{1+e^{-x}} \<br>→\frac{d\sigma(x)}{dx}&#x3D;\frac{e^{-x}}{(1+e^{-x})^2}&#x3D;(\frac{1+e^{-x}-1}{1+e^{-x}})(\frac{1}{1+e^{-x}})&#x3D;(1-\sigma(x))\sigma(x)<br>\end{array}<br>$$</p>
<p>举例</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic1.zhimg.com/0799b3d6e5e92245ee937db3c26d1b80_r.jpg"
                      alt="preview"
                ></p>
<p>实现代码</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">2</span>,-<span class="number">3</span>,-<span class="number">3</span>] <span class="comment"># 假设一些随机数据和权重</span></span><br><span class="line">x = [-<span class="number">1</span>, -<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">dot = w[<span class="number">0</span>]*x[<span class="number">0</span>] + w[<span class="number">1</span>]*x[<span class="number">1</span>] + w[<span class="number">2</span>]</span><br><span class="line">f = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-dot)) <span class="comment"># sigmoid函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对神经元反向传播</span></span><br><span class="line">ddot = (<span class="number">1</span> - f) * f <span class="comment"># 点积变量的梯度, 使用sigmoid函数求导</span></span><br><span class="line">dx = [w[<span class="number">0</span>] * ddot, w[<span class="number">1</span>] * ddot] <span class="comment"># 回传到x</span></span><br><span class="line">dw = [x[<span class="number">0</span>] * ddot, x[<span class="number">1</span>] * ddot, <span class="number">1.0</span> * ddot] <span class="comment"># 回传到w</span></span><br><span class="line"><span class="comment"># 完成！得到输入的梯度</span></span><br></pre></td></tr></table></figure></div>





<h6 id="分段计算"><a href="#分段计算" class="headerlink" title="分段计算"></a>分段计算</h6><p>首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子。<br>$$<br>f(x,y)&#x3D;\frac{x+\sigma(y)}{\sigma(x)+(x+y)^2}<br>$$</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">3</span> <span class="comment"># 例子数值</span></span><br><span class="line">y = -<span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">sigy = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-y)) <span class="comment"># 分子中的sigmoi          #(1)</span></span><br><span class="line">num = x + sigy <span class="comment"># 分子                                    #(2)</span></span><br><span class="line">sigx = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-x)) <span class="comment"># 分母中的sigmoid         #(3)</span></span><br><span class="line">xpy = x + y                                              <span class="comment">#(4)</span></span><br><span class="line">xpysqr = xpy**<span class="number">2</span>                                          <span class="comment">#(5)</span></span><br><span class="line">den = sigx + xpysqr <span class="comment"># 分母                                #(6)</span></span><br><span class="line">invden = <span class="number">1.0</span> / den                                       <span class="comment">#(7)</span></span><br><span class="line">f = num * invden <span class="comment"># 搞定！                                 #(8)</span></span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回传 f = num * invden</span></span><br><span class="line">dnum = invden <span class="comment"># 分子的梯度                                         #(8)</span></span><br><span class="line">dinvden = num                                                     <span class="comment">#(8)</span></span><br><span class="line"><span class="comment"># 回传 invden = 1.0 / den </span></span><br><span class="line">dden = (-<span class="number">1.0</span> / (den**<span class="number">2</span>)) * dinvden                                <span class="comment">#(7)</span></span><br><span class="line"><span class="comment"># 回传 den = sigx + xpysqr</span></span><br><span class="line">dsigx = (<span class="number">1</span>) * dden                                                <span class="comment">#(6)</span></span><br><span class="line">dxpysqr = (<span class="number">1</span>) * dden                                              <span class="comment">#(6)</span></span><br><span class="line"><span class="comment"># 回传 xpysqr = xpy**2</span></span><br><span class="line">dxpy = (<span class="number">2</span> * xpy) * dxpysqr                                        <span class="comment">#(5)</span></span><br><span class="line"><span class="comment"># 回传 xpy = x + y</span></span><br><span class="line">dx = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line">dy = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line"><span class="comment"># 回传 sigx = 1.0 / (1 + math.exp(-x))</span></span><br><span class="line">dx += ((<span class="number">1</span> - sigx) * sigx) * dsigx <span class="comment"># Notice += !! See notes below  #(3)</span></span><br><span class="line"><span class="comment"># 回传 num = x + sigy</span></span><br><span class="line">dx += (<span class="number">1</span>) * dnum                                                  <span class="comment">#(2)</span></span><br><span class="line">dsigy = (<span class="number">1</span>) * dnum                                                <span class="comment">#(2)</span></span><br><span class="line"><span class="comment"># 回传 sigy = 1.0 / (1 + math.exp(-y))</span></span><br><span class="line">dy += ((<span class="number">1</span> - sigy) * sigy) * dsigy                                 <span class="comment">#(1)</span></span><br><span class="line"><span class="comment"># 完成! 嗷~~</span></span><br></pre></td></tr></table></figure></div>

<p><strong>对前向传播变量进行缓存</strong>：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。</p>
<p><strong>在不同分支的梯度要相加</strong>：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用**+&#x3D;<strong>而不是</strong>&#x3D;*<em>来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的</em>多元链式法则*，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。</p>
<h5 id="门单元"><a href="#门单元" class="headerlink" title="门单元"></a>门单元</h5><p><strong>加法门单元</strong>把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。</p>
<p><strong>取最大值门单元</strong>对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了<strong>z</strong>变量，因为<strong>z</strong>的值比<strong>w</strong>高，于是<strong>w</strong>的梯度保持为0。</p>
<p><strong>乘法门单元</strong>的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。</p>
<h5 id="向量化操作"><a href="#向量化操作" class="headerlink" title="向量化操作"></a>向量化操作</h5><p><strong>矩阵相乘的梯度</strong>：可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">W = np.random.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">X = np.random.randn(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">D = W.dot(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们得到了D的梯度</span></span><br><span class="line">dD = np.random.randn(*D.shape) <span class="comment"># 和D一样的尺寸</span></span><br><span class="line">dW = dD.dot(X.T) <span class="comment">#.T就是对矩阵进行转置</span></span><br><span class="line">dX = W.T.dot(dD)</span><br></pre></td></tr></table></figure></div>

<p><em>提示：要分析维度！</em>注意不需要去记忆<strong>dW</strong>和<strong>dX</strong>的表达，因为它们很容易通过维度推导出来。例如，权重的梯度dW的尺寸肯定和权重矩阵W的尺寸是一样的，而这又是由<strong>X</strong>和<strong>dD</strong>的矩阵乘法决定的（在上面的例子中<strong>X</strong>和<strong>W</strong>都是数字不是矩阵）。总有一个方式是能够让维度之间能够对的上的。例如，<strong>X</strong>的尺寸是[10x3]，<strong>dD</strong>的尺寸是[5x3]，如果你想要dW和W的尺寸是[5x10]，那就要**dD.dot(X.T)**。</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>函数<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=max(0,-)"
                      alt="[公式]"
                >是非线性的，它会作用到每个元素。这个非线性函数有多种选择，后续将会学到。但这个形式是一个最常用的选择，它就是简单地设置阈值，将所有小于0的值变成0。</p>
<p>注意非线性函数在计算上是至关重要的，如果略去这一步，那么两个矩阵将会合二为一，对于分类的评分计算将重新变成关于输入的线性函数。这个非线性函数就是<em>改变</em>的关键点。参数<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=W_1,W_2"
                      alt="[公式]"
                >将通过随机梯度下降来学习到，他们的梯度在反向传播过程中，通过链式法则来求导计算得出。</p>
<p>计算公式<br>$$<br>s&#x3D;W_2max(0,W_1x) \<br>s&#x3D;W_3max(0,W_2max(0,W_1x))<br>$$</p>
<p>神经元前向传播实例代码</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="comment"># ... </span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">inputs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 假设输入和权重是1-D的numpy数组，偏差是一个数字 &quot;&quot;&quot;</span></span><br><span class="line">    cell_body_sum = np.<span class="built_in">sum</span>(inputs * self.weights) + self.bias</span><br><span class="line">    firing_rate = <span class="number">1.0</span> / (<span class="number">1.0</span> + math.exp(-cell_body_sum)) <span class="comment"># sigmoid激活函数</span></span><br><span class="line">    <span class="keyword">return</span> firing_rate</span><br></pre></td></tr></table></figure></div>

<p>换句话说，每个神经元都对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数（或称为激活函数）。本例中使用的是sigmoid函数<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Csigma(x)=1/(1+e%5E%7B-x%7D)"
                      alt="[公式]"
                >。在本节的末尾部分将介绍不同激活函数的细节。</p>
<h3 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h3><p><strong>二分类Softmax分类器</strong>。举例来说，可以把<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Csigma(%5CSigma_iw_ix_i+b)"
                      alt="[公式]"
                >看做其中一个分类的概率<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=P(y_i=1%7Cx_i;w)"
                      alt="[公式]"
                >，其他分类的概率为<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=P(y_i=0%7Cx_i;w)=1-P(y_i=1%7Cx_i;w)"
                      alt="[公式]"
                >，因为它们加起来必须为1。根据这种理解，可以得到交叉熵损失，这个在线性分一节中已经介绍。然后将它最优化为二分类的Softmax分类器（也就是逻辑回归）。因为sigmoid函数输出限定在0-1之间，所以分类器做出预测的基准是神经元的输出是否大于0.5。</p>
<p><strong>二分类SVM分类器</strong>。或者可以在神经元的输出外增加一个最大边界折叶损失（max-margin hinge loss）函数，将其训练成一个二分类的支持向量机。</p>
<p><strong>理解正则化</strong>。在SVM&#x2F;Softmax的例子中，正则化损失从生物学角度可以看做<em>逐渐遗忘</em>，因为它的效果是让所有突触权重<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=w"
                      alt="[公式]"
                >在参数更新过程中逐渐向着0变化。</p>
<blockquote>
<p>一个单独的神经元可以用来实现一个二分类分类器，比如二分类的Softmax或者SVM分类器。</p>
</blockquote>
<h3 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h3><h5 id="sigmoid函数-1"><a href="#sigmoid函数-1" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h5><p>sigmoid非线性函数的数学公式<br>$$<br>\sigma(x)&#x3D;1&#x2F;(1+e^{-x})<br>$$<br>函数图像（左）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic3.zhimg.com/80/677187e96671a4cac9c95352743b3806_720w.png"
                      alt="img"
                ></p>
<p>它输入实数值并将其“挤压”到0到1范围内。更具体地说，很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和（<strong>saturated</strong>）的激活（1）。然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点：</p>
<ul>
<li><em>Sigmoid函数饱和使梯度消失</em>。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。</li>
<li><em>Sigmoid函数的输出不是零中心的</em>。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f=w%5ETx+b"
                      alt="[公式]"
                >中每个元素都<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=x%3E0"
                      alt="[公式]"
                >），那么关于<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=w"
                      alt="[公式]"
                >的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f"
                      alt="[公式]"
                >而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。</li>
</ul>
<h5 id="Tanh函数"><a href="#Tanh函数" class="headerlink" title="Tanh函数"></a>Tanh函数</h5><p>它将实数值压缩到[-1,1]之间。和sigmoid神经元一样，它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，<em>tanh非线性函数比sigmoid非线性函数更受欢迎</em>。注意tanh神经元是一个简单放大的sigmoid神经元。<br>$$<br>tanh(x)&#x3D;2\sigma(2x)-1<br>$$<br>函数图像（右）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic3.zhimg.com/80/677187e96671a4cac9c95352743b3806_720w.png"
                      alt="img"
                ></p>
<h5 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h5><p>公式<br>$$<br>f(x)&#x3D;max(0,x)<br>$$<br>这个激活函数就是一个关于0的阈值。</p>
<p>图像（左）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic3.zhimg.com/80/83682a138f6224230f5b0292d9c01bd2_720w.png"
                      alt="img"
                ></p>
<p>左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=x=0"
                      alt="[公式]"
                >时函数值为0。当<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=x%3E0"
                      alt="[公式]"
                >函数的斜率为1。右边是从 <a class="link"   href="https://link.zhihu.com/?target=http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" >Krizhevsky <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。</p>
<p>relu函数在负半区的导数为0 ，所以一旦神经元激活值进入负半区，那么梯度就会为0，而正值不变，这种操作被成为单侧抑制。（也就是说：<strong>在输入是负值的情况下，它会输出0，那么神经元就不会被激活。这意味着同一时间只有部分神经元会被激活，从而使得网络很稀疏，进而对计算来说是非常有效率的。</strong>）正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。尤其体现在深度神经网络模型(如CNN)中，当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。</p>
<ul>
<li>优点：相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用（ <a class="link"   href="https://link.zhihu.com/?target=http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" >Krizhevsky  <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>等的论文指出有6倍之多）。据称这是由它的线性，非饱和的公式导致的。</li>
<li>优点：sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。</li>
<li>缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。ReLU 强制的稀疏处理会减少模型的有效容量（即特征屏蔽太多，导致模型无法学习到有效特征）。由于ReLU在x &lt; 0时梯度为0，这样就导致负的梯度在这个ReLU被置零，而且这个神经元有可能再也不会被任何数据激活，称为神经元“坏死”。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。</li>
</ul>
<h6 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h6><p>Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x&lt;0时，函数值为0。而Leaky ReLU则是给出一个很小的负数梯度值，比如0.01。所以其函数公式为<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f(x)=1(x%3C0)(%5Calpha+x)+1(x%3E=0)(x)"
                      alt="[公式]"
                >其中<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Calpha"
                      alt="[公式]"
                >是一个小的常量。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。Kaiming He等人在2015年发布的论文<a class="link"   href="https://link.zhihu.com/?target=http://arxiv.org/abs/1502.01852" >Delving Deep into Rectifiers <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>中介绍了一种新方法PReLU，把负区间上的斜率当做每个神经元中的一个参数。然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰。</p>
<h6 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h6><p>一些其他类型的单元被提了出来，它们对于权重和数据的内积结果不再使用<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f(w%5ETx+b)"
                      alt="[公式]"
                >函数形式。一个相关的流行选择是Maxout（最近由<a class="link"   href="https://link.zhihu.com/?target=http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html" >Goodfellow <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>等发布）神经元。Maxout是对ReLU和leaky ReLU的一般化归纳，它的函数是：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=max(w%5ET_1x+b_1,w%5ET_2x+b_2)"
                      alt="[公式]"
                >。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=w_1,b_1=0"
                      alt="[公式]"
                >的时候）。这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。</p>
<p><strong>一句话</strong>：“<em>那么该用那种呢？</em>”用ReLU非线性函数。注意设置好学习率，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。</p>
<h3 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h3><p>对于普通神经网络，最普通的层的类型是<strong>全连接层</strong>（fully-connected layer）。全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic3.zhimg.com/80/ccb56c1fb267bc632d6d88459eb14ace_720w.png"
                      alt="img"
                ></p>
<p>左边是一个2层神经网络，隐层由4个神经元（也可称为单元（unit））组成，输出层由2个神经元组成，输入层是3个神经元。右边是一个3层神经网络，两个含4个神经元的隐层。注意：层与层之间的神经元是全连接的，但是层内的神经元不连接。</p>
<p><strong>命名规则。</strong>当我们说N层神经网络的时候，我们没有把输入层算入。因此，单层的神经网络就是没有隐层的（输入直接映射到输出）。因此，有的研究者会说逻辑回归或者SVM只是单层神经网络的一个特例。研究者们也会使用<em>人工神经网络（</em>Artificial Neural Networks <em>缩写ANN）</em>或者<em>多层感知器（</em>Multi-Layer Perceptrons <em>缩写MLP</em>）来指代神经网络。很多研究者并不喜欢神经网络算法和人类大脑之间的类比，他们更倾向于用<em>单元（unit）</em>而不是神经元作为术语。</p>
<p><strong>输出层。</strong>和神经网络中其他层不同，输出层的神经元一般是不会有激活函数的（或者也可以认为它们有一个线性相等的激活函数）。这是因为最后的输出层大多用于表示分类评分值，因此是任意值的实数，或者某种实数值的目标数（比如在回归中）。</p>
<p><strong>确定网络尺寸。</strong>用来度量神经网络的尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数，用上面图示的两个网络举例：</p>
<ul>
<li>第一个网络有4+2&#x3D;6个神经元（输入层不算），[3x4]+[4x2]&#x3D;20个权重，还有4+2&#x3D;6个偏置，共26个可学习的参数。</li>
<li>第二个网络有4+4+1&#x3D;9个神经元，[3x4]+[4x4]+[4x1]&#x3D;32个权重，4+4+1&#x3D;9个偏置，共41个可学习的参数。</li>
</ul>
<h3 id="前向传播计算举例"><a href="#前向传播计算举例" class="headerlink" title="前向传播计算举例"></a>前向传播计算举例</h3><p><em>不断重复的矩阵乘法与激活函数交织</em>。将神经网络组织成层状的一个主要原因，就是这个结构让神经网络算法使用矩阵向量操作变得简单和高效。用上面那个3层神经网络举例，输入是[3x1]的向量。一个层所有连接的强度可以存在一个单独的矩阵中。比如第一个隐层的权重<strong>W1</strong>是[4x3]，所有单元的偏置储存在<strong>b1</strong>中，尺寸[4x1]。这样，每个神经元的权重都在<strong>W1</strong>的一个行中，于是矩阵乘法<strong>np.dot(W1, x)<strong>就能计算该层中所有神经元的激活数据。类似的，</strong>W2</strong>将会是[4x4]矩阵，存储着第二个隐层的连接，<strong>W3</strong>是[1x4]的矩阵，用于输出层。完整的3层神经网络的前向传播就是简单的3次矩阵乘法，其中交织着激活函数的应用。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个3层神经网络的前向传播:</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: <span class="number">1.0</span>/(<span class="number">1.0</span> + np.exp(-x)) <span class="comment"># 激活函数(用的sigmoid)</span></span><br><span class="line">x = np.random.randn(<span class="number">3</span>, <span class="number">1</span>) <span class="comment"># 含3个数字的随机输入向量(3x1)</span></span><br><span class="line">h1 = f(np.dot(W1, x) + b1) <span class="comment"># 计算第一个隐层的激活数据(4x1)</span></span><br><span class="line">h2 = f(np.dot(W2, h1) + b2) <span class="comment"># 计算第二个隐层的激活数据(4x1)</span></span><br><span class="line">out = np.dot(W3, h2) + b3 <span class="comment"># 神经元输出(1x1)</span></span><br></pre></td></tr></table></figure></div>

<p>在上面的代码中，<strong>W1，W2，W3，b1，b2，b3</strong>都是网络中可以学习的参数。注意<strong>x</strong>并不是一个单独的列向量，而可以是一个批量的训练数据（其中每个输入样本将会是<strong>x</strong>中的一列），所有的样本将会被并行化的高效计算出来。注意神经网络最后一层通常是没有激活函数的（例如，在分类任务中它给出一个实数值的分类评分）。</p>
<blockquote>
<p>全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。</p>
</blockquote>
<p>在实践中3层的神经网络会比2层的表现好，然而继续加深（做到4，5，6层）很少有太大帮助。卷积神经网络的情况却不同，在卷积神经网络中，对于一个良好的识别系统来说，深度是一个极端重要的因素（比如数十(以10为量级)个可学习的层）。对于该现象的一种解释观点是：因为图像拥有层次化结构（比如脸是由眼睛等组成，眼睛又是由边缘组成），所以多层处理对于这种数据就有直观意义。</p>
<h3 id="层的数量和尺寸"><a href="#层的数量和尺寸" class="headerlink" title="层的数量和尺寸"></a>层的数量和尺寸</h3><p>不要减少网络神经元数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练：虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。</p>
<p>在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>关于数据预处理我们有3个常用的符号，数据矩阵<strong>X</strong>，假设其尺寸是**[N x D]<strong>（</strong>N<strong>是数据样本的数量，</strong>D**是数据的维度）。</p>
<p><strong>均值减法</strong>（Mean subtraction)：它对数据中每个独立<em>特征</em>减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码**X -&#x3D; np.mean(X, axis&#x3D;0)<strong>实现。而对于图像，更常用的是对所有像素都减去一个值，可以用</strong>X -&#x3D; np.mean(X)**实现，也可以在3个颜色通道上分别操作。</p>
<p><strong>归一化</strong>(Normalization)：是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为**X &#x2F;&#x3D; np.std(X, axis&#x3D;0)**。第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic1.zhimg.com/80/e743b6777775b1671c3b5503d7afbbc4_720w.png"
                      alt="img"
                ></p>
<p><strong>左边：</strong>原始的2维输入数据。<strong>中间：</strong>在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。<strong>右边：</strong>每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。</p>
<p><font color=red><strong>PCA和白化</strong>：在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。</font></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设输入数据矩阵X的尺寸为[N x D]</span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># 对数据进行零中心化(重要)</span></span><br><span class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># 得到数据的协方差矩阵</span></span><br></pre></td></tr></table></figure></div>

<p>数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的<em>协方差</em>。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和<a class="link"   href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Positive-definite_matrix%23Negative-definite.2C_semidefinite_and_indefinite_matrices" >半正定 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>的。我们可以对数据协方差矩阵进行SVD（奇异值分解）运算。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">U,S,V = np.linalg.svd(cov)</span><br></pre></td></tr></table></figure></div>

<p>U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xrot = np.dot(X,U) <span class="comment"># 对数据去相关性</span></span><br></pre></td></tr></table></figure></div>

<p>注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算<strong>Xrot</strong>的协方差矩阵，将会看到它是对角对称的。<strong>np.linalg.svd</strong>的一个良好性质是在它的返回值<strong>U</strong>中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有<strong>方差</strong>的维度。 这个操作也被称为主成分分析（ <a class="link"   href="https://link.zhihu.com/?target=http://en.wikipedia.org/wiki/Principal_component_analysis" >Principal Component Analysis <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 简称PCA）降维：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment"># Xrot_reduced 变成 [N x 100]</span></span><br></pre></td></tr></table></figure></div>

<p>经过上面的操作，将原始的数据集的大小由[N x D]降到了[N x 100]，留下了数据中包含最大<strong>方差</strong>的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。</p>
<p>[^奇异值]: 是矩阵里的概念，一般通过奇异值分解定理求得。设A为m * n阶矩阵，q&#x3D;min(m,n)，A * A的q个非负特征值的算术平方根叫作A的奇异值。</p>
<p><strong>白化</strong>操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据进行白化操作:</span></span><br><span class="line"><span class="comment"># 除以特征值 </span></span><br><span class="line">Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure></div>

<p><em>警告：夸大的噪声</em>。注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic3.zhimg.com/80/aae11de6e6a29f50d46b9ea106fbb02a_720w.png"
                      alt="img"
                ></p>
<p>PCA&#x2F;白化。<strong>左边</strong>是二维的原始数据。<strong>中间</strong>：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。<strong>右边</strong>：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。</p>
<p>案例：<a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" >https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p><strong>实践操作。</strong>在这个笔记中提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。</p>
<p><strong>常见错误。</strong>进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练&#x2F;验证&#x2F;测试集，那么这个做法是错误的。<strong>应该怎么做呢？应该先分成训练&#x2F;验证&#x2F;测试集，只是从训练集中求图片平均值，然后各个集（训练&#x2F;验证&#x2F;测试集）中的图像再减去这个平均值。</strong></p>
<h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p><strong>错误：全零初始化</strong></p>
<p>如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。</p>
<p><strong>小随机数初始化</strong></p>
<p>将权重初始化为很小的数值，以此来打破对称性。</p>
<p>小随机数权重初始化的实现方法是：<strong>W &#x3D; 0.01 * np.random.randn(D,H)。</strong>其中<strong>randn</strong>函数是基于零均值和标准差的一个高斯分布（<em><strong>译者注：国内教程一般习惯称均值参数为期望 μ</strong></em>）来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。</p>
<!--警告。并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。-->

<p><strong>使用1&#x2F;sqrt(n)校准方差</strong></p>
<p>上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。这样就保证了网络中所有神经元起始时有近似同样的输出分布。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(n) / sqrt(n) <span class="comment"># 其中n是输入数据的数量</span></span><br></pre></td></tr></table></figure></div>

<p>推导过程此处略。</p>
<p><strong>依稀初始化</strong></p>
<p>另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。</p>
<p><strong>偏置的初始化</strong></p>
<p>通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。</p>
<p><strong>实践</strong></p>
<p>当前的推荐是使用ReLU激活函数，<a class="link"   href="https://link.zhihu.com/?target=http://arxiv-web3.library.cornell.edu/abs/1502.01852" >Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>，文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是 2.0&#x2F;n 。</p>
<p>代码为<code>w = np.random.randn(n) \* sqrt(2.0/n)</code></p>
<p>这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。（权重初始化）</p>
<p><strong>批量归一化</strong></p>
<p>让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层）与激活函数之间添加一个BatchNorm层。</p>
<p>批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。</p>
<h3 id="正则化-1"><a href="#正则化-1" class="headerlink" title="正则化"></a>正则化</h3><h5 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h5><p>可以通过惩罚目标函数中所有参数的平方将其实现。即对于网络中的每个权重<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=w"
                      alt="[公式]"
                >，向目标函数中增加一个<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5Clambda+w%5E2"
                      alt="[公式]"
                >，其中<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Clambda"
                      alt="[公式]"
                >是正则化强度。前面这个<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D"
                      alt="[公式]"
                >很常见，是因为加上<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D"
                      alt="[公式]"
                >后，该式子关于<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=w"
                      alt="[公式]"
                >梯度就是<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Clambda+w"
                      alt="[公式]"
                >而不是<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=2%5Clambda+w"
                      alt="[公式]"
                >了。</p>
<p>L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。在线性分类章节中讨论过，由于输入和权重之间的乘法操作，这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。最后需要注意在梯度下降和参数更新的时候，</p>
<p>使用L2正则化意味着所有的权重都以<strong>w +&#x3D; -lambda * W</strong>向着0线性下降。</p>
<h5 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h5><p>对于每个<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=w"
                      alt="[公式]"
                >我们都向目标函数增加一个<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Clambda%7Cw%7C"
                      alt="[公式]"
                >。L1和L2正则化也可以进行组合：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Clambda_1%7Cw%7C+%5Clambda_2w%5E2"
                      alt="[公式]"
                >，这也被称作<a class="link"   href="https://link.zhihu.com/?target=http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf" >Elastic net regularizaton <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>。L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。</p>
<h5 id="最大范式约束"><a href="#最大范式约束" class="headerlink" title="最大范式约束"></a>最大范式约束</h5><p>给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Coverrightarrow%7Bw%7D"
                      alt="[公式]"
                >必须满足<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%7C%7C%5Coverrightarrow%7Bw%7D%7C%7C_2%3Cc"
                      alt="[公式]"
                >这一条件，一般<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=c"
                      alt="[公式]"
                >值为3或者4。有研究者发文称在使用这种正则化方法时效果更好。这种正则化还有一个良好的性质，即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的。</p>
<h5 id="随机失活"><a href="#随机失活" class="headerlink" title="随机失活"></a>随机失活</h5><p>与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=p"
                      alt="[公式]"
                >的概率被激活或者被设置为0。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic4.zhimg.com/80/63fcf4cc655cb04f21a37e86aca333cf_720w.png"
                      alt="img"
                ></p>
<p>在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。</p>
<h6 id="普通版随机失活"><a href="#普通版随机失活" class="headerlink" title="普通版随机失活"></a>普通版随机失活</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; 普通版随机失活: 不推荐实现 (看下面笔记) &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">X</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot; X中是输入数据 &quot;&quot;&quot;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  U1 = np.random.rand(*H1.shape) &lt; p <span class="comment"># 第一个随机失活遮罩</span></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  U2 = np.random.rand(*H2.shape) &lt; p <span class="comment"># 第二个随机失活遮罩</span></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X</span>):</span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure></div>

<p>上述操作不好的性质是必须在测试时对激活数据要按照<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=p"
                      alt="[公式]"
                >进行数值范围调整。既然测试性能如此关键，实际更倾向使用<strong>反向随机失活（inverted dropout）</strong>，它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。</p>
<h6 id="反向随机失活"><a href="#反向随机失活" class="headerlink" title="反向随机失活"></a>反向随机失活</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">反向随机失活: 推荐实现方式.</span></span><br><span class="line"><span class="string">在训练的时候drop和调整数值范围，测试时不做任何事.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">X</span>):</span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 第一个随机失活遮罩. 注意/p!</span></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># 第二个随机失活遮罩. 注意/p!</span></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X</span>):</span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># 不用数值范围调整了</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure></div>



<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p><strong>前向传播中的噪音</strong>。测试时，通过<em>分析法</em>（在使用随机失活的本例中就是乘以<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=p"
                      alt="[公式]"
                >）或<em>数值法</em>（例如通过抽样出很多子网络，随机选择不同子网络进行前向传播，最后对它们取平均）将噪音边缘化。在这个方向上的另一个研究是<a class="link"   href="https://link.zhihu.com/?target=http://cs.nyu.edu/~wanli/dropc/" >DropConnect <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>，它在前向传播的时候，一系列权重被随机设置为0。</p>
<p><strong>偏置正则化。</strong>在线性分类器的章节中介绍过，对于偏置参数的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差。这可能是因为相较于权重参数，偏置参数实在太少，所以分类器需要它们来获得一个很好的数据损失，那么还是能够承受的。</p>
<p><strong>每层正则化。</strong>对于不同的层进行不同强度的正则化很少见（可能除了输出层以外），关于这个思路的相关文献也很少。</p>
<p><strong>实践</strong>：通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=p"
                      alt="[公式]"
                >值一般默认设为0.5，也可能在验证集上调参。</p>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数：SVM函数、softmax分类器</p>
<h5 id="类别数目巨大"><a href="#类别数目巨大" class="headerlink" title="类别数目巨大"></a>类别数目巨大</h5><p>当标签集非常庞大，就需要使用分层softmax。分层softmax将标签分解为一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。</p>
<h5 id="属性分类-不太明白"><a href="#属性分类-不太明白" class="headerlink" title="属性分类(不太明白)"></a>属性分类(不太明白)</h5><p>上述两个损失函数的前提为假设每个样本只有一个正确的标签<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=y_i"
                      alt="[公式]"
                >。如果<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=y_i"
                      alt="[公式]"
                >是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间并不相互排斥。</p>
<p>在这种情况下，一个明智的方法是为每个属性创建一个独立的<strong>二分类的分类器</strong>。</p>
<p>例如，针对每个分类的二分类器会采用下面的公式：<br>$$<br>L_i&#x3D;\sum_jmax(0,1-y_{ij}f_j)<br>$$<br>上式中，求和是对所有分类<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=j"
                      alt="[公式]"
                >，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=y_%7Bij%7D"
                      alt="[公式]"
                >的值为1或者-1，具体根据第i个样本是否被第j个属性打标签而定，当该类别被正确预测并展示的时候，分值向量<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f_j"
                      alt="[公式]"
                >为正，其余情况为负。可以发现，当一个正样本的得分小于+1，或者一个负样本得分大于-1的时候，算法就会累计损失值。</p>
<p>另一种方法是对每种属性训练一个独立的<strong>逻辑回归分类器</strong>。</p>
<p>二分类的逻辑回归分类器只有两个分类（0，1），其中对于分类1的概率计算为：<br>$$<br>P(y&#x3D;1|x;w,b)&#x3D;\frac{1}{1+e^{-(w^Tx+b)}}&#x3D;\sigma(w^Tx+b)<br>$$<br>因为类别0和类别1的概率和为1，所以类别0的概率为：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+P(y=0%7Cx;w,b)=1-P(y=1%7Cx;w,b)"
                      alt="[公式]"
                >。这样，如果<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Csigma(w%5ETx+b)%3E0.5"
                      alt="[公式]"
                >或者<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=w%5ETx+b%3E0"
                      alt="[公式]"
                >，那么样本就要被分类成为正样本（y&#x3D;1）。然后损失函数最大化这个对数似然函数，问题可以简化为：<br>$$<br>L_i&#x3D;\sum_jy_{ij}log(\sigma(f_j))+(1-y_{ij})log(1-\sigma(f_j))<br>$$<br>上式中，假设标签<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=y_%7Bij%7D"
                      alt="[公式]"
                >非0即1，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Csigma(.)"
                      alt="[公式]"
                >就是sigmoid函数。上面的公式看起来吓人，但是<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f"
                      alt="[公式]"
                >的梯度实际上非常简单：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+f_j%7D=y_%7Bij%7D-%5Csigma(f_j)"
                      alt="[公式]"
                ></p>
<h5 id="回归问题-L1不明白"><a href="#回归问题-L1不明白" class="headerlink" title="回归问题(L1不明白)"></a>回归问题(L1不明白)</h5><p>预测实数的值的问题。通常是计算预测值和真实值之间的损失。然后用L2平方范式或L1范式度量差异。</p>
<p>对于某个样本，L2范式计算如下：</p>
<p>（进行平方因为平方是一个单调运算，不用改变最优参数，梯度算起来更加简单）<br>$$<br>L_i&#x3D;||f-y_i||^2_2<br>$$<br>L1范式则是要将每个维度上的绝对值加起来：<br>$$<br>L_i&#x3D;||f-y_i||<em>1&#x3D;\sum_j|f_j-(y_i)<em>j|<br>$$<br>在上式中，如果有多个数量被预测了，就要对预测的所有维度的预测求和，即<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Csum_j"
                      alt="[公式]"
                >。观察第i个样本的第j维，用![[公式]](<a class="link"   target="_blank" rel="noopener" href="https://www.zhihu.com/equation?tex=%5Cdelta" >https://www.zhihu.com/equation?tex=%5Cdelta <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></em>%7Bij%7D)表示预测值与真实值之间的差异。关于该维度的梯度（也就是<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cpartial+L_i/%5Cpartial+f_j"
                      alt="[公式]"
                >）能够轻松地通过被求导为L2范式的![[公式]](<a class="link"   target="_blank" rel="noopener" href="https://www.zhihu.com/equation?tex=%5Cdelta" >https://www.zhihu.com/equation?tex=%5Cdelta <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></em>%7Bij%7D)或<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=sign(%5Cdelta_%7Bij%7D)"
                      alt="[公式]"
                >。这就是说，评分值的梯度要么与误差中的差值直接成比例，要么是固定的并从差值中继承sign。</p>
<h3 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h3><p><strong>使用中心化公式</strong><br>$$<br>\lim_{h→0}\frac{f(x+h)-f(x-h)}{2h}<br>$$<br><strong>使用相对误差</strong></p>
<p>假设这个差值是1e-4，如果两个梯度值在1.0左右，这个差值看起来就很合适，可以认为两个梯度是匹配的。然而如果梯度值是1e-5或者更低，那么1e-4就是非常大的差距，梯度实现肯定就是失败的了。<br>$$<br>\frac{|f’_a-f’_n|}{max(|f’_a|,|f’_n|)}<br>$$<br>上式考虑了差值占两个梯度绝对值的比例。注意通常相对误差公式只包含两个式子中的一个（任意一个均可），但是我更倾向取两个式子的最大值或者取两个式子的和。这样做是为了防止在其中一个式子为0时，公式分母为0（这种情况，在ReLU中是经常发生的）。然而，还必须注意两个式子都为零且通过梯度检查的情况。在实践中：</p>
<ul>
<li>相对误差&gt;1e-2：通常就意味着梯度可能出错。</li>
<li>1e-2&gt;相对误差&gt;1e-4：要对这个值感到不舒服才行。</li>
<li>1e-4&gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高。</li>
<li>1e-7或者更小：好结果，可以高兴一把了。</li>
</ul>
<p>要知道的是网络的深度越深，相对误差就越高。所以如果你是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。相反，如果一个可微函数的相对误差值是1e-2，那么通常说明梯度实现不正确。</p>
<p><strong>使用双精度</strong></p>
<p>一个常见的错误是使用单精度浮点数来进行梯度检查。这样会导致即使梯度实现正确，相对误差值也会很高（比如1e-2）。在我的经验而言，出现过使用单精度浮点数时相对误差为1e-2，换成双精度浮点数时就降低为1e-8的情况。</p>
<p><strong>保持在浮点数的有效范围</strong></p>
<p>确保用来比较的数字的值不是过小。</p>
<p><strong>目标函数的不可导点（kinks）</strong></p>
<p>在进行梯度检查时，一个导致不准确的原因是不可导点问题。不可导点是指目标函数不可导的部分，由ReLU（<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=max(0,x)"
                      alt="[公式]"
                >）等函数，或SVM损失，Maxout神经元等引入。考虑当<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=x=-1e6"
                      alt="[公式]"
                >的时，对ReLU函数进行梯度检查。因为<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=x%3C0"
                      alt="[公式]"
                >，所以解析梯度在该点的梯度为0。然而，在这里数值梯度会突然计算出一个非零的梯度值，因为<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f(x+h)"
                      alt="[公式]"
                >可能越过了不可导点(例如：如果<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=h%3E1e-6"
                      alt="[公式]"
                >)，导致了一个非零的结果。</p>
<p>注意，在计算损失的过程中是可以知道不可导点有没有被越过的。在具有<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=max(x,y)"
                      alt="[公式]"
                >形式的函数中持续跟踪所有“赢家”的身份，就可以实现这一点。其实就是看在前向传播时，到底x和y谁更大。如果在计算<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f(x+h)"
                      alt="[公式]"
                >和<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f(x-h)"
                      alt="[公式]"
                >的时候，至少有一个“赢家”的身份变了，那就说明不可导点被越过了，数值梯度会不准确。</p>
<p><strong>使用少量数据点</strong></p>
<p>解决上面的不可导点问题的一个办法是<font color=CadetBlue>使用更少的数据点</font>。因为含有不可导点的损失函数(例如：因为使用了ReLU或者边缘损失等函数)的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。</p>
<p>如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。</p>
<p><strong>谨慎设置步长h</strong></p>
<p>在实践中h并不是越小越好，因为当<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=h"
                      alt="[公式]"
                >特别小的时候，就可能就会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=h"
                      alt="[公式]"
                >调到1e-4或者1e-6，然后突然梯度检查可能就恢复正常。</p>
<p><strong>在操作的特性模式中梯度检查</strong></p>
<p>梯度检查是在参数空间中的一个特定（往往还是随机的）的单独点进行的。即使是在该点上梯度检查成功了，也不能马上确保全局上梯度的实现都是正确的。还有，一个随机的初始化可能不是参数空间最优代表性的点，这可能导致进入某种病态的情况，即梯度看起来是正确实现了，实际上并没有。</p>
<p>例如，SVM使用小数值权重初始化，就会把一些接近于0的得分分配给所有的数据点，而梯度将会在所有的数据点上展现出某种模式。一个不正确实现的梯度也许依然能够产生出这种模式，但是不能泛化到更具代表性的操作模式，比如在一些的得分比另一些得分更大的情况下就不行。</p>
<p>因此为了安全起见，最好让网络学习（“预热”）一小段时间，<font color=CadetBlue>等到损失函数开始下降的之后再进行梯度检查</font>。在第一次迭代就进行梯度检查的危险就在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。</p>
<p><strong>不要让正则化吞没数据</strong></p>
<p>通常损失函数是数据损失和正则化损失的和（例如L2对权重的惩罚）。需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分（正则化部分的梯度表达式通常简单很多）。这样就会掩盖掉数据损失梯度的不正确实现。因此，<font color=CadetBlue>推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查</font>。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。</p>
<p><strong>记得关闭随机失活（dropout）和数据扩张（augmentation）</strong></p>
<p>在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。关闭这些操作不好的一点是无法对它们进行梯度检查（例如随机失活的反向传播实现可能有错误）。因此，一个更好的解决方案就是<font color=CadetBlue>在计算<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f(x+h)"
                      alt="[公式]"
                >和<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=f(x-h)"
                      alt="[公式]"
                >前强制增加一个特定的随机种子，在计算解析梯度时也同样如此</font>。</p>
<p><strong>检查少量的维度</strong></p>
<p>在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。</p>
<p><strong>注意</strong>：确认在所有不同的参数中都抽取一部分来梯度检查。在某些应用中，为了方便，人们将所有的参数放到一个巨大的参数向量中。在这种情况下，例如偏置就可能只占用整个向量中的很小一部分，所以不要随机地从向量中取维度，一定要把这种情况考虑到，确保所有参数都收到了正确的梯度。</p>
<h3 id="合理化检查"><a href="#合理化检查" class="headerlink" title="合理化检查"></a>合理化检查</h3><p>在进行费时费力的最优化之前，最好进行一些合理性检查：</p>
<ul>
<li><p><strong>寻找特定情况的正确损失值。</strong>在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。</p>
<p>例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)&#x3D;2.302。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。</p>
</li>
<li><p>第二个合理性检查：提高正则化强度时导致损失值变大。</p>
</li>
<li><p><strong>对小数据子集过拟合。</strong>最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。除非能通过这一个正常性检查，不然进行整个数据集训练是没有意义的。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。</p>
</li>
</ul>
<h3 id="检查学习过程"><a href="#检查学习过程" class="headerlink" title="检查学习过程"></a>检查学习过程</h3><p>在下面的图表中，x轴通常都是表示<strong>周期（epochs）</strong>单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。</p>
<p><strong>损失函数</strong></p>
<p>训练期间第一个要跟踪的数值就是损失值，它在前向传播时对每个独立的批数据进行计算。下图展示的是随着损失值随时间的变化，尤其是曲线形状会给出关于学习率设置的情况：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic2.zhimg.com/80/753f398b46cc28c1916d6703cf2080f5_720w.png"
                      alt="img"
                ></p>
<p><strong>左图</strong>展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。</p>
<p><strong>右图</strong>显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。</p>
<p>损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。</p>
<p><strong>训练集和验证机准确率</strong></p>
<p>在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。这个图表能够展现知道模型过拟合的程度：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic3.zhimg.com/80/05a6960a01c0204ced8d875ac3d91fba_720w.jpg"
                      alt="img"  
                >



<p>在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。</p>
<p><strong>权重更新比例</strong></p>
<p>最后一个应该跟踪的量是权重中更新值的数量和全部值的数量之间的比例。注意：是<em>更新的</em>，而不是原始梯度（比如，在普通sgd中就是梯度乘以学习率）。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设参数向量为W，其梯度向量为dW</span></span><br><span class="line">param_scale = np.linalg.norm(W.ravel())</span><br><span class="line">update = -learning_rate*dW <span class="comment"># 简单SGD更新</span></span><br><span class="line">update_scale = np.linalg.norm(update.ravel())</span><br><span class="line">W += update <span class="comment"># 实际更新</span></span><br><span class="line"><span class="built_in">print</span> update_scale / param_scale <span class="comment"># 要得到1e-3左右</span></span><br></pre></td></tr></table></figure></div>

<p><strong>每层的激活数据及梯度分布</strong></p>
<p>一个不正确的初始化可能让学习过程变慢，甚至彻底停止。还好，这个问题可以比较简单地诊断出来。其中一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。</p>
<p><strong>第一层可视化</strong></p>
<p>最后，如果数据是图像像素数据，那么把第一层特征可视化会有帮助：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic3.zhimg.com/80/96573094f9d7f4b3b188069726840a2e_720w.png"
                      alt="img"
                ></p>
<p>将神经网络第一层的权重可视化的例子。<strong>左图</strong>中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。<strong>右图</strong>的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。</p>
<h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><h5 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h5><p><strong>普通更新</strong></p>
<p>最简单的更新形式是沿着负梯度方向改变参数（因为梯度指向的是上升方向，但是我们通常希望最小化损失函数）。假设有一个参数向量<strong>x</strong>及其梯度<strong>dx</strong>，那么最简单的更新的形式是：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通更新</span></span><br><span class="line">x += - learning_rate * dx</span><br></pre></td></tr></table></figure></div>

<p>其中learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。</p>
<p><strong>动量更新</strong></p>
<p>这个方法在深度网络上几乎总能得到更好的收敛速度。该方法可以看成是从物理角度上对于最优化问题得到的启发。损失值可以理解为是山的高度（因此高度势能是<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=U=mgh"
                      alt="[公式]"
                >，所以有<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=U%5Cpropto+h"
                      alt="[公式]"
                >）。用随机数字初始化参数等同于在某个位置给质点设定初始速度为0。这样最优化过程可以看做是模拟参数向量（即质点）在地形上滚动的过程。</p>
<p>因为作用于质点的力与梯度的潜在能量（<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=F=-%5Cnabla+U"
                      alt="[公式]"
                >）有关，质点<strong>所受的力</strong>就是损失函数的<strong>（负）梯度</strong>。还有，因为<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=F=ma"
                      alt="[公式]"
                >，所以在这个观点下（负）梯度与质点的加速度是成比例的。注意这个理解和上面的随机梯度下降（SDG）是不同的，在普通版本中，梯度直接影响位置。而在这个版本的更新中，物理观点建议梯度只是影响速度，然后速度再影响位置：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动量更新</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># 与速度融合</span></span><br><span class="line">x += v <span class="comment"># 与位置融合</span></span><br></pre></td></tr></table></figure></div>

<p>在这里引入了一个初始化为0的变量<strong>v</strong>和一个超参数<strong>mu</strong>。说得不恰当一点，这个变量（mu）在最优化的过程中被看做<em>动量</em>（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火（下文有讨论）类似，动量随时间变化的设置有时能略微改善最优化的效果，其中动量在学习过程的后阶段会上升。一个典型的设置是刚开始将动量设为0.5而在后面的多个周期（epoch）中慢慢提升到0.99。</p>
<blockquote>
<p>通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。</p>
</blockquote>
<p><strong>Nesterov动量</strong></p>
<p>与普通动量有些许不同，最近变得比较流行。在理论上对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。</p>
<p>Nesterov动量的核心思路是，当参数向量位于某个位置<strong>x</strong>时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过<strong>mu * v</strong>稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置<strong>x + mu * v</strong>看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，计算<strong>x + mu * v</strong>的梯度而不是“旧”位置<strong>x</strong>的梯度就有意义了。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic1.zhimg.com/80/412afb713ddcff0ba9165ab026563304_720w.png"
                      alt="img"
                ></p>
<p>Nesterov动量。既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个“向前看”的地方计算梯度。</p>
<p>也就是说，添加一些注释后，实现代码如下：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_ahead = x + mu * v</span><br><span class="line"><span class="comment"># 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)</span></span><br><span class="line">v = mu * v - learning_rate * dx_ahead</span><br><span class="line">x += v</span><br></pre></td></tr></table></figure></div>

<p>然而在实践中，人们更喜欢和普通SGD或上面的动量方法一样简单的表达式。通过对<strong>x_ahead &#x3D; x + mu * v</strong>使用变量变换进行改写是可以做到的，然后用<strong>x_ahead</strong>而不是<strong>x</strong>来表示上面的更新。也就是说，实际存储的参数向量总是向前一步的那个版本。<strong>x_ahead</strong>的公式（将其重新命名为<strong>x</strong>）就变成了：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_prev = v <span class="comment"># 存储备份</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># 速度更新保持不变</span></span><br><span class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># 位置更新变了形式</span></span><br></pre></td></tr></table></figure></div>



<h5 id="学习率退火"><a href="#学习率退火" class="headerlink" title="学习率退火"></a>学习率退火</h5><p>如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。知道什么时候开始衰减学习率是有技巧的：慢慢减小它，可能在很长时间内只能是浪费计算资源地看着它混沌地跳动，实际进展很少。但如果快速地减少它，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有3种方式：</p>
<ul>
<li><strong>随步数衰减</strong>：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。</li>
<li><strong>指数衰减</strong>。数学公式是<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Calpha=%5Calpha_0e%5E%7B-kt%7D"
                      alt="[公式]"
                >，其中<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Calpha_0,k"
                      alt="[公式]"
                >是超参数，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=t"
                      alt="[公式]"
                >是迭代次数（也可以使用周期作为单位）。</li>
<li><strong>1&#x2F;t衰减</strong>的数学公式是<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Calpha=%5Calpha_0/(1+kt)"
                      alt="[公式]"
                >，其中<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Calpha_0,k"
                      alt="[公式]"
                >是超参数，t是迭代次数。</li>
</ul>
<p>在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=k"
                      alt="[公式]"
                >更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。</p>
<h5 id="二阶方法"><a href="#二阶方法" class="headerlink" title="二阶方法"></a>二阶方法</h5><p>在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下：<br>$$<br>x←x-[Hf(x)]^{-1}\nabla f(x)<br>$$<br>这里<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=Hf(x)"
                      alt="[公式]"
                >是<a class="link"   href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Hessian_matrix" >Hessian矩阵 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>，它是函数的二阶偏导数的平方矩阵。<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Cnabla+f(x)"
                      alt="[公式]"
                >是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。</p>
<p><strong>实践</strong>。在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。</p>
<h3 id="逐参数适应学习率方法"><a href="#逐参数适应学习率方法" class="headerlink" title="逐参数适应学习率方法"></a>逐参数适应学习率方法</h3><h5 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设有梯度和参数向量x</span></span><br><span class="line">cache += dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure></div>

<p>注意，变量<strong>cache</strong>的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数的梯度的平方和。这个一会儿将用来归一化参数更新步长，归一化是逐元素进行的。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强。有趣的是平方根的操作非常重要，如果去掉，算法的表现将会糟糕很多。用于平滑的式子<strong>eps</strong>（一般设为1e-4到1e-8之间）是防止出现除以0的情况。Adagrad的一个缺点是，在深度学习中单调的学习率被证明通常过于激进且过早停止学习。</p>
<h5 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h5><p>这个方法用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cache =  decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure></div>

<p>在上面的代码中，decay_rate是一个超参数，常用的值是[0.9,0.99,0.999]。其中<strong>x+&#x3D;<strong>和Adagrad中是一样的，但是</strong>cache</strong>变量是不同的。因此，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这同样效果不错。但是和Adagrad不同，其更新不会让学习率单调变小。</p>
<h5 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h5><p>看起来像是RMSProp的动量版。简化的代码是下面这样：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = beta1 * m + (<span class="number">1</span>-beta1)*dw</span><br><span class="line">mt = m / (<span class="number">1</span>-beta1**t)</span><br><span class="line">v = beta2 * v + (<span class="number">1</span>-beta2)*(dw ** <span class="number">2</span>)</span><br><span class="line">vt = v / (<span class="number">1</span>-beta2 ** t)</span><br><span class="line">next_w = w - learning_rate * mt / (np.sqrt(vt) + eps )</span><br></pre></td></tr></table></figure></div>

<p>注意这个更新方法看起来真的和RMSProp很像，除了使用的是平滑版的梯度<strong>m</strong>，而不是用的原始梯度向量<strong>dx</strong>。论文中推荐的参数值<strong>eps&#x3D;1e-8, beta1&#x3D;0.9, beta2&#x3D;0.999</strong>。在实际操作中，我们推荐Adam作为默认的算法，一般而言跑起来比RMSProp要好一点。但是也可以试试SGD+Nesterov动量。完整的Adam更新算法也包含了一个偏置（bias）矫正机制，因为<strong>m,v</strong>两个矩阵初始为0，在没有完全热身之前存在偏差，需要采取一些补偿措施。</p>
<h3 id="超参数调优"><a href="#超参数调优" class="headerlink" title="超参数调优"></a>超参数调优</h3><p>我们已经看到，训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有：</p>
<ul>
<li>初始学习率。</li>
<li>学习率衰减方式（例如一个衰减常量）。</li>
<li>正则化强度（L2惩罚，随机失活强度）。</li>
</ul>
<p><strong>实现</strong>。更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要，因为这会影响你设计代码的思路。一个具体的设计是用<strong>仆程序</strong>持续地随机设置参数然后进行最优化。在训练过程中，<strong>仆程序</strong>会对每个周期后验证集的准确率进行监控，然后向文件系统写下一个模型的记录点（记录点中有各种各样的训练统计数据，比如随着时间的损失值变化等），这个文件系统最好是可共享的。在文件名中最好包含验证集的算法表现，这样就能方便地查找和排序了。然后还有一个<strong>主程序</strong>，它可以启动或者结束计算集群中的<strong>仆程序</strong>，有时候也可能根据条件查看<strong>仆程序</strong>写下的记录点，输出它们的训练统计数据等。</p>
<p><strong>比起交叉验证最好使用一个验证集</strong>。在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。你可能会听到人们说他们“交叉验证”一个参数，但是大多数情况下，他们实际是使用的一个验证集。</p>
<p><strong>超参数范围</strong>。在对数尺度上进行超参数搜索。例如，一个典型的学习率应该看起来是这样：*<em>learning_rate &#x3D; 10 *</em> uniform(-6, 1)<strong>。也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有乘的效果。例如：当学习率是0.001的时候，如果对其固定地增加0.01，那么对于学习进程会有很大影响。然而当学习率是10的时候，影响就微乎其微了。这就是因为学习率乘以了计算出的梯度。因此，比起加上或者减少某些值，思考学习率的范围是乘以或者除以某些值更加自然。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：</strong>dropout&#x3D;uniform(0,1)**）。</p>
<p><strong>随机搜索优于网格搜索</strong>。Bergstra和Bengio在文章<a class="link"   href="https://link.zhihu.com/?target=http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" >Random Search for Hyper-Parameter Optimization <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>中说“随机选择比网格化的选择更加有效”，而且在实践中也更容易实现。通常，有些超参数比其余的更重要，通过随机搜索，而不是网格化的搜索，可以让你更精确地发现那些比较重要的超参数的好数值。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic4.zhimg.com/80/d25cf561835c7b96ae6d1c91868bcbff_720w.png"
                      alt="img"
                ></p>
<p><strong>对于边界上的最优值要小心</strong>。这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使用*<em>learning_rate &#x3D; 10 *</em> uniform(-6,1)**来进行搜索。一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。</p>
<p><strong>从粗到细地分阶段搜索</strong>。在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练一个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。</p>
<p><strong>贝叶斯超参数最优化</strong>是一整个研究领域，主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。基于这些模型，发展出很多的库，比较有名的有： <a class="link"   href="https://link.zhihu.com/?target=https://github.com/JasperSnoek/spearmint" >Spearmint <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>, <a class="link"   href="https://link.zhihu.com/?target=http://www.cs.ubc.ca/labs/beta/Projects/SMAC/" >SMAC <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>, 和<a class="link"   href="https://link.zhihu.com/?target=http://jaberg.github.io/hyperopt/" >Hyperopt <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>。然而，在卷积神经网络的实际使用中，比起上面介绍的先认真挑选的一个范围，然后在该范围内随机搜索的方法，这个方法还是差一些。<a class="link"   href="https://link.zhihu.com/?target=http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html" >这里 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>有更详细的讨论。</p>
<h3 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h3><h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法：</p>
<ul>
<li><strong>同一个模型，不同的初始化</strong>。使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。这种方法的风险在于多样性只来自于不同的初始化条件。</li>
<li><strong>在交叉验证中发现最好的模型</strong>。使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。</li>
<li><strong>一个模型设置多个记录点</strong>。如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。</li>
<li><strong>在训练的时候跑参数的平均值</strong>。和上面一点相关的，还有一个也能得到1-2个百分点的提升的小代价方法，这个方法就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样你就对前几次循环中的网络状态进行了平均。你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。</li>
</ul>
<p>模型集成的一个劣势就是在测试数据的时候会花费更多时间。最近Geoff Hinton在“<a class="link"   href="https://link.zhihu.com/?target=https://www.youtube.com/watch?v=EK61htlw8hY" >Dark Knowledge <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>”上的工作很有启发：其思路是通过将集成似然估计纳入到修改的目标函数中，从一个好的集成中抽出一个单独模型。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>训练一个神经网络需要：</p>
<ul>
<li>利用小批量数据对实现进行梯度检查，还要注意各种错误。</li>
<li>进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。</li>
<li>在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数量相对于总参数量的比例（一般在1e-3左右），然后如果是对于卷积神经网络，可以将第一层的权重可视化。</li>
<li>推荐的两个更新方法是SGD+Nesterov动量方法，或者Adam方法。</li>
<li>随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。</li>
<li>使用随机搜索（不要用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。</li>
<li>进行模型集成来获得额外的性能提高。</li>
</ul>
<h2 id="卷积神经网络CNNs"><a href="#卷积神经网络CNNs" class="headerlink" title="卷积神经网络CNNs"></a>卷积神经网络CNNs</h2><p>卷积神经网络和常规神经网络非常相似：</p>
<ul>
<li>它们都是由神经元组成，神经元中有具有学习能力的权重和偏差。</li>
<li>每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。</li>
<li>整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出是不同类别的评分。</li>
<li>在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或Softmax）</li>
<li>并且在神经网络中我们实现的各种技巧和要点依旧适用于卷积神经网络。</li>
</ul>
<p>那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。</p>
<h3 id="结构概述"><a href="#结构概述" class="headerlink" title="结构概述"></a>结构概述</h3><p>神经网络的输入是一个向量，然后在一系列的<em>隐层</em>中对它做变换。每个隐层都是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接。但是在一个隐层中，神经元相互独立不进行任何连接。最后的全连接层被称为“输出层”，在分类问题中，它输出的值被看做是不同类别的评分值。</p>
<p><strong>常规神经网络对于大尺寸图像效果不尽人意</strong>。在CIFAR-10中，图像的尺寸是32x32x3（宽高均为32像素，3个颜色通道），因此，对应的的常规神经网络的第一个隐层中，每一个单独的全连接神经元就有32x32x3&#x3D;3072个权重。这个数量看起来还可以接受，但是很显然这个全连接的结构不适用于更大尺寸的图像。举例说来，一个尺寸为200x200x3的图像，会让神经元包含200x200x3&#x3D;120,000个权重值。而网络中肯定不止一个神经元，那么参数的量就会快速增加！显而易见，这种全连接方式效率低下，大量的参数也很快会导致网络过拟合。</p>
<p><strong>神经元的三维排列</strong>。卷积神经网络针对输入全部是图像的情况，将结构调整得更加合理，获得了不小的优势。与常规神经网络不同，卷积神经网络的各层中的神经元是3维排列的：<strong>宽度</strong>、<strong>高度</strong>和<strong>深度</strong>（这里的<strong>深度</strong>指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）。举个例子，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是32x32x3（宽度，高度和深度）。我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的。下面是例子：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic2.zhimg.com/80/2ef08bb4cf60805d726b2d6db39dd985_720w.jpg"
                      alt="img"
                ></p>
<p>左边是一个3层的神经网络。右边是一个卷积神经网络，图例中网络将它的神经元都排列成3个维度（宽、高和深度）。卷积神经网络的每一层都将3D的输入数据变化为神经元3D的激活数据并输出。在这个例子中，红色的输入层装的是图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是3（代表了红、绿、蓝3种颜色通道）。</p>
<blockquote>
<p>卷积神经网络是由层组成的。每一层都有一个简单的API：用一些含或者不含参数的可导的函数，将输入的3D数据变换为3D的输出数据。</p>
</blockquote>
<p><strong>用来构建卷积网络的各种层</strong></p>
<p>一个简单的卷积神经网络是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。卷积神经网络主要由三种类型的层构成：<strong>卷积层</strong>，<strong>汇聚（Pooling）层</strong>和<strong>全连接层</strong>（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。</p>
<p><em>网络结构例子：</em>这仅仅是个概述，下面会更详解的介绍细节。一个用于CIFAR-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下：</p>
<ul>
<li><strong>输入</strong>[32x32x3]存有图像的原始像素值，本例中图像宽高均为32，有3个颜色通道。</li>
<li><strong>卷积层</strong>中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积。卷积层会计算所有神经元的输出。如果我们使用12个滤波器（也叫作核），得到的输出数据体的维度就是[32x32x12]。</li>
<li><strong>ReLU层</strong>将会逐个元素地进行激活函数操作，比如使用以0为阈值的max(0,x)作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。</li>
<li><strong>汇聚层</strong>在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。</li>
<li><strong>全连接层</strong>将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。</li>
</ul>
<p>由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值。其中有的层含有参数，有的没有。具体说来，卷积层和全连接层（CONV&#x2F;FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。</p>
<p><strong>小结</strong>：</p>
<ul>
<li>简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）。</li>
<li>卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）。</li>
<li>每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据。</li>
<li>有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。</li>
<li>有的层有额外的超参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）</li>
</ul>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p><a class="link"   target="_blank" rel="noopener" href="https://www.datakit.cn/blog/2016/03/23/bp_cnn.html" >https://www.datakit.cn/blog/2016/03/23/bp_cnn.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>卷积层的参数是有一些可学习的<strong>滤波器</strong>集合构成的。在前向传播的失活，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。直观来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活。</p>
<p>每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的<strong>感受野（receptive field）</strong>，它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。</p>
<p><em>例1</em>：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3&#x3D;75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。</p>
<p><strong>空间排列</strong>：上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：<strong>深度（depth），步长（stride）</strong>和<strong>零填充（zero-padding）</strong>。下面是对它们的讨论：</p>
<ol>
<li>首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为<strong>深度列（depth column）</strong>，也有人使用纤维（fibre）来称呼它们。</li>
<li>其次，在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。</li>
<li>在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的。这个<strong>零填充（zero-padding）</strong>的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。</li>
</ol>
<p>输出数据体在空间上的尺寸可以通过输入数据体尺寸（W），卷积层中神经元的感受野尺寸（F），步长（S）和零填充的数量（P）的函数来计算。</p>
<p>输出数据体的空间尺寸为(W-F +2P)&#x2F;S+1。（设定时让其为整数）</p>
<p>比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出。如果步长为2，输出就是3x3。</p>
<p><strong>深度切片</strong>：作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量。比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]。在每个深度切片上的神经元都使用同样的权重和偏差。</p>
<p>在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。</p>
<p>注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的<strong>卷积</strong>（这就是“卷积层”名字由来）。这也是为什么总是将这些权重集合称为<strong>滤波器（filter）</strong>（或<strong>卷积核（kernel）</strong>），因为它们和输入进行了卷积。</p>
<p><strong>局部连接层</strong>（Locally-Connected Layer）：注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。</p>
<p>一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为局部连接层。</p>
<p><strong>卷积层的性质</strong></p>
<ul>
<li><p>输入数据体的尺寸为W1×H1×D1</p>
</li>
<li><p>4个超参数：</p>
</li>
<li><ul>
<li>滤波器的数量K</li>
<li>滤波器的空间尺寸F</li>
<li>步长S</li>
<li>零填充数量P</li>
</ul>
</li>
<li><p>输出数据体的尺寸为W2×H2×D2 ，其中：</p>
</li>
<li><ul>
<li>W2&#x3D;(W1−F+2P)&#x2F;S+1</li>
<li>H2&#x3D;(H1−F+2P)&#x2F;S+1 （宽度和高度的计算方法相同）</li>
</ul>
</li>
</ul>
<p>D2&#x3D;K</p>
<ul>
<li>由于参数共享，每个滤波器包含F⋅F⋅D1个权重，卷积层一共有F⋅F⋅D1⋅K个权重和K个偏置。</li>
<li>在输出数据体中，第d个深度切片（空间尺寸是W2×H2），用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长S），最后在加上第d个偏差。</li>
</ul>
<p>对这些超参数，常见的设置是F&#x3D;3，S&#x3D;1，P&#x3D;1。同时设置这些超参数也有一些约定俗成的惯例和经验，可以在下面的卷积神经网络结构章节中查看。</p>
<p><strong>用矩阵乘法实现</strong>：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法：</p>
<ol>
<li>输入图像的局部区域被<strong>im2col</strong>操作拉伸为列。比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3&#x3D;363的列向量。重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)&#x2F;4+1&#x3D;55，所以得到<em>im2col</em>操作的输出矩阵<strong>X_col</strong>的尺寸是[363x3025]，其中每列是拉伸的感受野，共有55x55&#x3D;3,025个。注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。</li>
<li>卷积层的权重也同样被拉伸成行。举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵<strong>W_row</strong>，尺寸为[96x363]。</li>
<li>现在卷积的结果和进行一个大矩阵乘**np.dot(W_row, X_col)**是等价的了，能得到每个滤波器和每个感受野间的点积。在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出。</li>
<li>结果最后必须被重新变为合理的输出尺寸[55x55x96]。</li>
</ol>
<p>这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在<strong>X_col</strong>中被复制了多次。但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的<a class="link"   href="https://link.zhihu.com/?target=http://www.netlib.org/blas/" >BLAS <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> API）。还有，同样的<em>im2col</em>思路可以用在汇聚操作中。</p>
<p><strong>1x1卷积</strong>：这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）。</p>
<p><strong>扩张卷积</strong>：最近一个研究（<a class="link"   href="https://link.zhihu.com/?target=https://arxiv.org/abs/1511.07122" >Fisher Yu和Vladlen Koltun的论文 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）给卷积层引入了一个新的叫<em>扩张（dilation）</em>的超参数。到目前为止，我们只讨论了卷积层滤波器是连续的情况。但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张。举例，在某个维度上滤波器<strong>w</strong>的尺寸是3，那么计算输入<strong>x</strong>的方式是：**w[0]*x[0] + w[1]*x[1] + w[2]*x[2]**，此时扩张为0。如果扩张为1，那么计算为： **w[0]*x[0] + w[1]*x[2] + w[2]*x[4]*<em>。换句话说，操作中存在1的间隙。在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为在很少的层数内更快地汇集输入图片的大尺度特征。比如，如果上下重叠2个3x3的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中5x5的区域（可以成这些神经元的</em>有效感受野*是5x5）。如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长。</p>
<h3 id="汇聚层（池化层）"><a href="#汇聚层（池化层）" class="headerlink" title="汇聚层（池化层）"></a>汇聚层（池化层）</h3><p>通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。</p>
<p>汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。</p>
<p>汇聚层的一些公式：</p>
<ul>
<li><p>输入数据体尺寸W1⋅H1⋅D1</p>
</li>
<li><p>有两个超参数：</p>
</li>
<li><ul>
<li>空间大小F</li>
<li>步长S</li>
</ul>
</li>
<li><p>输出数据体尺寸W2⋅H2⋅D2，其中</p>
<p>W2&#x3D;(W1−F)&#x2F;S+1</p>
<p>H2&#x3D;(H1−F)&#x2F;S+1</p>
<p>D2&#x3D;D1</p>
</li>
<li><p>因为对输入进行的是固定函数计算，所以没有引入参数</p>
</li>
<li><p>在汇聚层中很少使用零填充</p>
</li>
</ul>
<p>在实践中，最大汇聚层通常只有两种形式：一种是F&#x3D;3,S&#x3D;2，也叫<strong>重叠汇聚（overlapping pooling）</strong>，另一个更常用的是F&#x3D;2,S&#x3D;2。对更大感受野进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性。</p>
<p><strong>普通汇聚（General Pooling）</strong>：除了最大汇聚，汇聚单元还可以使用其他的函数，比如<em>平均</em>汇聚<em>（average pooling）</em>或<em>L-2范式</em>汇聚<em>（L2-norm pooling）</em>。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic4.zhimg.com/80/641c8846abcb02d35938660cf96cef1b_720w.jpg"
                      alt="img"
                ></p>
<p>汇聚层在输入数据体的每个深度切片上，独立地对其进行空间上的降采样。左边：本例中，输入数据体尺寸[224x224x64]被降采样到了[112x112x64]，采取的滤波器尺寸是2，步长为2，而深度不变。右边：最常用的降采样操作是取最大值，也就是最大汇聚，这里步长为2，每个取最大值操作是从4个数字中选取（即2x2的方块区域中）。</p>
<p><strong>反向传播：</strong>回顾一下反向传播的内容，其中max(x,y)函数的反向传播可以简单理解为将梯度只沿最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大元素的索引记录下来（有时这个也叫作<strong>道岔（switches）</strong>），这样在反向传播的时候梯度的路由就很高效。</p>
<p><strong>不使用汇聚层</strong>：很多人不喜欢汇聚操作，认为可以不使用它。比如在<a class="link"   href="https://link.zhihu.com/?target=http://arxiv.org/abs/1412.6806" >Striving for Simplicity: The All Convolutional Net <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。</p>
<h3 id="归一层"><a href="#归一层" class="headerlink" title="归一层"></a>归一层</h3><p>在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。对于不同类型的归一化层，可以看看Alex Krizhevsky的关于<a class="link"   href="https://link.zhihu.com/?target=https://code.google.com/p/cuda-convnet/wiki/LayerParams%23Local_response_normalization_layer_(same_map)" >cuda-convnet library API <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>的讨论。</p>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看<em>神经网络</em>章节。</p>
<p><strong>把全连接层转化成卷积层</strong></p>
<p>全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：</p>
<ul>
<li>对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块（这是因为有局部连接），其余部分都是零。而在其中大部分块中，元素都是相等的（因为参数共享）。</li>
<li>相反，任何全连接层都可以被转化为卷积层。比如，一个K&#x3D;4096的全连接层，输入数据体的尺寸是7×7×512，这个全连接层可以被等效地看做一个F&#x3D;7,P&#x3D;0,S&#x3D;1,K&#x3D;4096的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成1×1×4096，这个结果就和使用初始的那个全连接层一样了。</li>
</ul>
<p><strong>全连接层转化为卷积层</strong>：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224&#x2F;2&#x2F;2&#x2F;2&#x2F;2&#x2F;2&#x3D;7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：</p>
<ul>
<li>针对第一个连接区域是[7x7x512]的全连接层，<font color=CadetBlue>令其滤波器尺寸为F&#x3D;7</font>，这样输出数据体就为[1x1x4096]了。</li>
<li>针对第二个全连接层，令其滤波器尺寸为F&#x3D;1，这样输出数据体为[1x1x4096]。</li>
<li>对最后一个全连接层也做类似的，令其F&#x3D;1，最终输出为[1x1x1000]</li>
</ul>
<p>实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（<strong>译者注</strong>：即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。</p>
<p>举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384&#x2F;2&#x2F;2&#x2F;2&#x2F;2&#x2F;2 &#x3D; 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)&#x2F;1 + 1 &#x3D; 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！（<em><strong>译者注</strong>：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解</em>）</p>
<blockquote>
<p>面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。</p>
</blockquote>
<p>自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。</p>
<p>最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。（<em><strong>译者注</strong>：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解</em>）</p>
<ul>
<li><a class="link"   href="https://link.zhihu.com/?target=https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb" >Net Surgery <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>上一个使用Caffe演示如何在进行变换的IPython Note教程。</li>
</ul>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>卷积神经网络通常是由三种层构成：卷积层(CONV)，汇聚层（除非特别说明，一般就是最大值汇聚, POOL）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。</p>
<h4 id="层的排列规律"><a href="#层的排列规律" class="headerlink" title="层的排列规律"></a>层的排列规律</h4><p>最常见的卷积神经网络结构如下：</p>
<p><strong>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC</strong></p>
<p>其中<em>*<em><strong>指的是重复次数，</strong>POOL?<strong>指的是一个可选的汇聚层。其中</strong>N &gt;&#x3D;0</em>*,通常</em><em>N&lt;&#x3D;3**,**M&gt;&#x3D;0</em>*,<strong>K&gt;&#x3D;0</strong>,通常<strong>K&lt;3</strong>。</p>
<p>例如，下面是一些常见的网络结构规律：</p>
<ul>
<li><strong>INPUT -&gt; FC</strong>,实现一个线性分类器，此处<strong>N &#x3D; M &#x3D; K &#x3D; 0</strong>。</li>
<li><strong>INPUT -&gt; CONV -&gt; RELU -&gt; FC</strong></li>
<li><strong>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC</strong>。此处在每个汇聚层之间有一个卷积层。</li>
<li><strong>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC</strong>。此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。</li>
</ul>
<p><strong>几个小滤波器卷积层的组合比一个大滤波器卷积层好</strong>：</p>
<p>假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。</p>
<p>假设不采用这3个3x3的卷积层，而是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7。</p>
<p>缺点：首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有C个通道，那么单独的7x7卷积层将会包含C×(7×7×C)&#x3D;49C²个参数，而3个3x3的卷积层的组合仅有3×(C×(3×3×C))&#x3D;27C²个参数。</p>
<p>直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。</p>
<h4 id="层的尺寸设置规律"><a href="#层的尺寸设置规律" class="headerlink" title="层的尺寸设置规律"></a>层的尺寸设置规律</h4><p><strong>输入层</strong>（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。</p>
<p><strong>卷积层</strong>应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长S&#x3D;1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当F&#x3D;3，那就使用P&#x3D;1来保持输入尺寸。当F&#x3D;5,P&#x3D;2，一般对于任意F，当P&#x3D;(F−1)&#x2F;2的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上。</p>
<p><strong>汇聚层</strong>负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野（即F&#x3D;2）的最大值汇聚，步长为2（S&#x3D;2）。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。</p>
<p><em>减少尺寸设置的问题</em>：上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。</p>
<p><em>为什么在卷积层使用1的步长</em>？在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。</p>
<p><em>为何使用零填充</em>？使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。</p>
<p><em>因为内存限制所做的妥协</em>：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。</p>
<h4 id="计算上的考量"><a href="#计算上的考量" class="headerlink" title="计算上的考量"></a>计算上的考量</h4><p>在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈。大部分现代GPU的内存是3&#x2F;4&#x2F;6GB，最好的GPU大约有12GB的内存。要注意三种内存占用来源：</p>
<ul>
<li>来自中间数据体尺寸：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）。通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）。在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到。但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量。</li>
<li>来自参数尺寸：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存。因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多。</li>
<li>卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等。</li>
</ul>
<p>一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位。把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量。如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了。</p>
<h2 id="循环神经网络RNNs"><a href="#循环神经网络RNNs" class="headerlink" title="循环神经网络RNNs"></a>循环神经网络RNNs</h2><h3 id="RNN网络结构"><a href="#RNN网络结构" class="headerlink" title="RNN网络结构"></a>RNN网络结构</h3><p>CNN在对样本提取特征的时候，样本与样本之间是独立的，而有些情况是无法把每个输入的样本都看作是独立的，这类问题可以看作一种带有时序序列的问题，单纯的用CNN和DNN解决就比较棘手，此时RNN是一种解决这类问题很好的模型。</p>
<p><strong>公式</strong></p>
<p>我们可以通过在每一时步应用一个循环的公式来处理一系列的x向量，fw是权重W的一些函数，xt是每个时步输入的向量，ht为新的状态：<br>$$<br>h_t&#x3D;f_W(h_{t-1},x_t)<br>$$<br>注意：每个时步都使用相同的函数和相同的参数</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20220728141015233.png"
                      alt="image-20220728141015233" style="zoom:50%;" 
                >



<p><strong>Vanilla RNN</strong>：把隐藏态ht公式进行修改，yt例如分类评分的预测，Why为另一个权重：<br>$$<br>h_t&#x3D;tanh(W_{hh}h_{t-1}+W_{xh}x_t)<br>$$</p>
<p>$$<br>y_t&#x3D;W_{hy}h_t<br>$$</p>
<p><strong>RNN计算图</strong></p>
<p>h0初始值设为0，W是所有时步下独立计算出来的梯度之和。</p>
<p>损失函数在w上的梯度，最终的损失值又会回溯到每一个时步的损失，然后每一个时步又会各自计算出在权重w上的梯度，它们的总和就是权重w的最终梯度。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/10.png"
                      alt="img"
                ></p>
<p><strong>处理序列</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20220728134844261.png"
                      alt="image-20220728134844261"
                ></p>
<p>一对多：图片字幕，图片 -&gt; 文字序列；	多对一：情绪分类，文字序列-&gt;情绪</p>
<p>多对多：机器翻译，文字序列-&gt;文字序列；	可变序列：每一帧的视频分类</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/11.png"
                      alt="img" style="zoom: 33%;" 
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/12.png"
                      alt="img" style="zoom: 33%;" 
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/13.png"
                      alt="img" style="zoom:33%;" 
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/14.png"
                      alt="img" style="zoom:33%;" 
                >



<p><strong>反向传播</strong></p>
<p>通过反向传播向前遍历整个序列来计算损失，并遍历整个序列来计算梯度。如果我们选择整个序列，这将会变得非常慢并且耗费大量内存并永不收敛。因此人们采用<strong>Truncated反向传播</strong>，将序列分为几块来单独计算前向传播和反向传播，然后将隐藏状态永远向前传递，但只会反向传播一些较少的步骤。</p>
<p>时序反向传播算法（BPTT）</p>
<p>沿时间截断反向传播算法</p>
<p>截断式BPTT（Truncated BPTT）是完整BPTT的近似方法，是处理是长序列的首选。在时间步较多的序列中，完整BPTT的每个参数更新的正向&#x2F;反向运算成本变得非常高。截断式BPTT的缺点是，由于截断，梯度反向移动的距离有限，因此网络无法学习与完整BPTT一样长的依赖。</p>
<p><strong>LSTM</strong></p>
<p>多层RNNs通常使用一些层作为隐藏层并再次输入。</p>
<p>梯度爆炸通常由梯度截断来控制，梯度消失则更改RNN的结构，通过加性相互作用（LSTM）来控制。</p>
<p><strong>Vanilla RNN梯度流动</strong>，最大奇异值大于1时会产生梯度爆炸，最大奇异值小于1时会产生梯度消失。<br>$$<br>\begin{align}<br>h_t &amp; &#x3D; tanh(W_{hh}h_{t-1}+W_{xh}x_t) \<br>&amp; &#x3D;tanh \begin{pmatrix}(W_{hh} &amp;W_{hx})<br>\begin{pmatrix} h_{t-1} \ x_t<br>\end{pmatrix} \end{pmatrix} \<br>&amp;&#x3D;tanh \begin{pmatrix} W\begin{pmatrix} h_{t-1} \x_t<br>\end{pmatrix}\end{pmatrix}<br>\end{align} \<br>$$<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20220728151108816.png"
                      alt="image-20220728151108816"
                ></p>
<p><strong>长短期记忆</strong>（LSTM，Long Short Term Memory）</p>
<p>LSTM有助于保留可以通过时间和层进行反向传播的误差。LSTM将信息存放在循环网络正常信息流之外的门控单元中。信息可以像计算机内存中的数据一样存储、写入单元，或者从单元中读取。单元通过打开和关闭的门来决定存储什么，以及何时允许读取、写入和忘记。但与计算机上的数字存储器不同，这些门是模拟的，通过范围在0~1之间的sigmoid函数的逐元素相乘来实现。与数字信号相比，模拟信号的优势是可微分，因此适用于反向传播。</p>
<p>ct为单元状态，保留LSTM内部的隐藏状态，并且不会完全暴露到外部去。<br>$$<br>\begin{pmatrix} i \ f\o\g \end{pmatrix} &#x3D;<br>\begin{pmatrix} \sigma \ \sigma \ \sigma \ tanh \end{pmatrix} W<br>\begin{pmatrix} h_{t-1} \ x_t \end{pmatrix}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;c_t&#x3D;f \odot c_{t-1}+i \odot g \<br>&amp;h_t &#x3D; o \odot tanh(c_t)<br>\end{align}<br>$$</p>
<p>f : Forget gate, Whether to erase cell</p>
<p>i : Input gate, Whether to write to cell</p>
<p>g : Gate gate (?), How much to write to cell</p>
<p>o : Output gate, How much to reveal cell</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/18.png"
                      alt="img"
                ></p>
<p>优点：</p>
<ol>
<li>矩阵元素相乘，而不是矩阵相乘</li>
<li>可能会在不同的时间点乘以一个不同的遗忘门，避免了梯度爆炸或者消失</li>
<li>sigmoid函数相乘的结果会在0到1之间，使数值性质更好</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20220728154007052.png"
                      alt="image-20220728154007052"
                ></p>
<h3 id="识别和分割"><a href="#识别和分割" class="headerlink" title="识别和分割"></a>识别和分割</h3><p>第一个方法是使用<strong>滑动窗口</strong>，我们采用一个小窗口尺寸，并将其在图片上滑动，对于每一个窗口我们想要给中心像素标记。但它需要巨大的计算量，并且十分无效、不重用重叠图块之间的共享特征。实际过程中并不被采纳使用。</p>
<p>第二个方法是<strong>全连接卷积网络</strong>，从图像中提取各个图像块，并且分类，可以把网络当成很多的卷积层堆叠在一起，并同时对像素进行预测。输入整个图像并得到带有每个像素标记的图像的输出。这需要一个深度卷积层，且数据增强很好，但这需要大量的标记数据，且以原始图像分辨率进行卷积非常昂贵，</p>
<p>第三个方法是基于上一个想法，不同之处在于我们在网络内部进行了下采样，之后对特征做上采样。这仅仅只对一部分卷积层做原清晰度处理，之后下采样特征，不采用全连接层</p>
<p>我们下采样是因为使用整个图像非常昂贵。</p>
<p>下采样是一种类似于最大池化和跨卷积的操作。</p>
<p>上采样就像“最近邻”或“钉床函数”或“最大去池化”。</p>
<p><strong>最大去池化</strong></p>
<p>池化会让我们损失很多细节，因此最大去池化帮助我们很好处理细节，帮助我们存储空间信息（在最大池化的时候丢失的那些）。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20220728191010720.png"
                      alt="image-20220728191010720"
                ></p>
<p><strong>最近邻和钉床函数</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20220728190941669.png"
                      alt="image-20220728190941669"
                ></p>
<p><strong>卷积转置</strong></p>
<p>可以用来做上采样，即上采样特征图，又能学习权重来做上采样。一个卷积操作是一个多对一(many-to-one)的映射关系。现在，假设我们想要反过来操作。将其变成一个一对多(one-to-many)的映射关系。这个就像是卷积操作的反操作，其核心观点就是用转置卷积。</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/67907490#:~:text=%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%20%28Transposed%20Convolution%29%E5%B8%B8%E5%B8%B8%E5%9C%A8%E4%B8%80%E4%BA%9B%E6%96%87%E7%8C%AE%E4%B8%AD%E4%B9%9F%E7%A7%B0%E4%B9%8B%E4%B8%BA%E5%8F%8D%E5%8D%B7%E7%A7%AF%20%28Deconvolution%29%E5%92%8C%E9%83%A8%E5%88%86%E8%B7%A8%E8%B6%8A%E5%8D%B7%E7%A7%AF,%28Fractionally-strided%20Convolution%29%EF%BC%8C%E5%9B%A0%E4%B8%BA%E7%A7%B0%E4%B9%8B%E4%B8%BA%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%AE%B9%E6%98%93%E8%AE%A9%E4%BA%BA%E4%BB%A5%E4%B8%BA%E5%92%8C%E6%95%B0%E5%AD%97%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E4%B8%AD%E5%8F%8D%E5%8D%B7%E7%A7%AF%E6%B7%B7%E8%B5%B7%E6%9D%A5%EF%BC%8C%E9%80%A0%E6%88%90%E4%B8%8D%E5%BF%85%E8%A6%81%E7%9A%84%E8%AF%AF%E8%A7%A3%EF%BC%8C%E5%9B%A0%E6%AD%A4%E4%B8%8B%E6%96%87%E9%83%BD%E5%B0%86%E7%A7%B0%E4%B8%BA%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%BB%BA%E8%AE%AE%E5%90%84%E4%BD%8D%E4%B8%8D%E8%A6%81%E9%87%87%E7%94%A8%E5%8F%8D%E5%8D%B7%E7%A7%AF%E8%BF%99%E4%B8%AA%E7%A7%B0%E5%91%BC%E3%80%82.%20%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E6%83%B3%E8%A6%81%E6%88%91%E4%BB%AC%E7%9A%84%E7%BD%91%E7%BB%9C%E5%8F%AF%E4%BB%A5%E5%AD%A6%E4%B9%A0%E5%88%B0%E6%9C%80%E5%A5%BD%E5%9C%B0%E4%B8%8A%E9%87%87%E6%A0%B7%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E6%88%91%E4%BB%AC%E8%BF%99%E4%B8%AA%E6%97%B6%E5%80%99%E5%B0%B1%E5%8F%AF%E4%BB%A5%E9%87%87%E7%94%A8%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E3%80%82.%20%E8%BF%99%E4%B8%AA%E6%96%B9%E6%B3%95%E4%B8%8D%E4%BC%9A%E4%BD%BF%E7%94%A8%E9%A2%84%E5%85%88%E5%AE%9A%E4%B9%89%E7%9A%84%E6%8F%92%E5%80%BC%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AE%83%E5%85%B7%E6%9C%89%E5%8F%AF%E4%BB%A5%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%82%E6%95%B0%E3%80%82.%20%E7%90%86%E8%A7%A3%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E8%BF%99%E4%B8%AA%E6%A6%82%E5%BF%B5%E6%98%AF%E5%BE%88%E9%87%8D%E8%A6%81%E7%9A%84%EF%BC%8C%E5%9B%A0%E4%B8%BA%E5%AE%83%E5%9C%A8%E8%8B%A5%E5%B9%B2%E9%87%8D%E8%A6%81%E7%9A%84%E6%96%87%E7%8C%AE%E4%B8%AD%E9%83%BD%E6%9C%89%E6%89%80%E5%BA%94%E7%94%A8%EF%BC%8C%E5%A6%82%EF%BC%9A." >转置卷积 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>跨卷积：不依次向下放置过滤单元，而是每隔两个像素单位做一次，在输入中，过滤器（卷积核）每次移动一个格，输出也每次移动一格。</p>
<p><strong>候选区域网络</strong></p>
<p>找到物体可能存在的备选区域，再应用卷积神经网络对这些备选区域进行分类</p>
<ul>
<li>查找可能包含对象的斑点图像区域</li>
<li>运行速度相对较快：例如在CPU上几秒钟选择性搜索，提供1000个区域建议</li>
</ul>
<p><strong>分类和定位问题</strong></p>
<p>该技术可用于许多其他问题，例如：人体姿势估计。在这个问题中，我们要将图像中的主要对象及其位置分类为矩形。<br>我们假设有一个对象，我们将创建一个多任务 NN。通常第一个 Conv 层是预训练的 NN，例如 AlexNet。 架构如下：</p>
<p>卷积网络层连接到：</p>
<ul>
<li>对对象进行分类的 FC 层。 （我们知道的简单分类问题）</li>
<li>连接到四个数字 (x,y,w,h) 的 FC 层</li>
</ul>
<p>我们将本地化视为回归问题，这个问题会有两个损失，损失 &#x3D; SoftmaxLoss + L2 损失：</p>
<ul>
<li>分类的 Softmax 损失</li>
<li>本地化（L2 损失）的回归（线性损失）</li>
</ul>
<p><strong>R-CNN</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/20.png"
                      alt="img"
                ></p>
<p>需要训练的参数：</p>
<ul>
<li>使用 softmax 分类器微调网络（对数损失）</li>
<li>训练事后线性 SVM（铰链损失）</li>
<li>训练事后边界框回归（最小二乘）</li>
</ul>
<p>这个想法很糟糕，因为它采用区域建议的图像的一部分，如果不同的大小，并在将它们全部缩放到一个大小后将其提供给 CNN。它在训练和识别的时候也很慢。</p>
<p><strong>Fast R-CNN</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/48.png"
                      alt="img"
                ></p>
<p>Faster R-CNN 通过插入区域提议网络（RPN）来做自己的区域提议，以预测来自特征的提议。通过四个损失值来训练：</p>
<ul>
<li>RPN分类物体或非物体</li>
<li>RPN回归物体边界</li>
<li>最终的分类分数（物体分类）</li>
<li>最终的物体边界</li>
</ul>
<p>另外，Faster R-CNN更慢但更准确，SSD&#x2F;YOLO更快，但不那么准确。</p>
<p><strong>Mask R-CNN</strong></p>
<p>我们不想预测边界框，而是想知道哪个像素标签，并且将他们分离出来。</p>
<p>我们需要具备两个特定属性的生成图像：</p>
<p>第一个属性是，我们想要最大程度地激活一些分值或者神经元的值</p>
<p>第二个属性是，我们希望这个生成图像看起来是自然的</p>
<h3 id="可视化理解"><a href="#可视化理解" class="headerlink" title="可视化理解"></a>可视化理解</h3><p>第一种方法是可视化第一层的过滤器。事实证明，这些过滤器可以像人脑一样学习原始形状和定向边缘。虽然可以可视化来自下层的过滤器，但没有办法得到任何信息。</p>
<p><strong>Maximally Activating Patches</strong></p>
<p>帮助可视化卷积层中的中间特征，我们选择一个层然后选择一个神经元。通过网络运行多个图像，记录所选通道的值，可视化最大激活的图像部分，会发现每个神经元都在查看图像的特定部分，使用感受野提取图像。例如我们在AlexNet中选择Conv5，它是128x13x13，然后选择通道17&#x2F;128.</p>
<p><strong>区块遮挡</strong></p>
<p>在输入CNN之前屏蔽部分图像，在每个屏蔽位置绘制概率热图，观察是图像的哪个部分影响了网络的判断。将原图片分成若干个相同大小的区块，分别将其置为灰色、跑神经网络，看哪些区块的影响最为明显，可以绘制出热力图观察。</p>
<p>需要注意的是，热力图的冷色调区域代表响应比较明显，暖色调区域代表响应比较弱</p>
<p>不过唯一的问题就是每个区块的大小，不宜太大，否则可能损失热力图的精度；也不宜太小，否则可能增加可视化的时间，而且毕竟神经网络的容错性很强，丢掉很重要部分的某几个像素可能也不影响结果。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/mbadry1/CS231n-2017-Summary/master/Images/51.png"
                      alt="img"
                ></p>
<p><strong>嵌入空间可视化</strong></p>
<p>这种思路通常会使用<strong>CNN</strong>中全连接层的第一层<strong>feature层</strong>、<strong>RNN</strong>中<strong>词向量层</strong>完成可视化。这是一个高维向量可视化的任务，通常使用PCA、t-SNE等降维方法将高维空间降低到二维或三维，然后将每一个样本点或词向量对应的嵌入空间中的点标注出来，绘制出散点图。下图是某种网络结构下mnist数据集的表现：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic1.zhimg.com/80/v2-ab028874d83229781b5c8cd9f3a9d6f4_720w.jpg"
                      alt="img"
                ></p>
<p>训练初期测试集的嵌入空间（降维到二维），可以看出非常杂乱</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic3.zhimg.com/80/v2-544b089740395a57c21fa585d573702e_720w.jpg"
                      alt="img"
                ></p>
<p>训练末期测试集的嵌入空间（降维到二维），不同类别之间有了明确的界限</p>
<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>自动编码器</p>
<p>变分自编码器</p>
<h3 id="深度增强学习"><a href="#深度增强学习" class="headerlink" title="深度增强学习"></a>深度增强学习</h3><p>推断算法</p>
<ul>
<li>剪枝</li>
<li>共享权值</li>
<li>量化</li>
<li>Winograd二元化和三元化</li>
</ul>
<p>高效推断的硬件</p>
<ul>
<li>TPU，充分利用8位整型</li>
<li>EIE加速器，充分利用稀疏性，任何数乘以0都是0，所以不需要储存或者计算</li>
</ul>
<p>训练的高效算法</p>
<ul>
<li>如何执行并行化</li>
<li>关于如何利用16位精度浮点数的优势来进行混合精度训练，而不是完全使用32位精度浮点训练</li>
<li>使用更好的稀疏正则项的 密-疏-密 训练，和 老师-学生 模型</li>
</ul>
<p>高效训练的硬件</p>
<ul>
<li>Volta CPU</li>
<li>进化版的TPU、云TPU以及在Nvidia GPU最新一代中的Tensor cores</li>
</ul>
<h2 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h2><h3 id="涉及词汇"><a href="#涉及词汇" class="headerlink" title="涉及词汇"></a>涉及词汇</h3><h5 id="训练函数、预测函数"><a href="#训练函数、预测函数" class="headerlink" title="训练函数、预测函数"></a>训练函数、预测函数</h5><p>训练函数：接收图片和标签，输出模型</p>
<p>预测函数：接受一个模型，对图片种类进行预测</p>
<h5 id="最近邻近算法"><a href="#最近邻近算法" class="headerlink" title="最近邻近算法"></a>最近邻近算法</h5><p>K-邻近算法：根据距离度量，找到最近的K个点，在这些相邻点进行投票预测</p>
<h5 id="分类回归"><a href="#分类回归" class="headerlink" title="分类回归"></a>分类回归</h5><p>分类 间断 交叉熵损失 一般采用softmax函数或向量机的边界损失</p>
<p>回归 连续 L1、L2</p>
<h5 id="图片分类"><a href="#图片分类" class="headerlink" title="图片分类"></a>图片分类</h5><p>训练集 train</p>
<p>验证集 validation</p>
<p>测试集 test</p>
<h5 id="欧氏距离"><a href="#欧氏距离" class="headerlink" title="欧氏距离"></a>欧氏距离</h5><p>也称欧几里得距离，是最常见的距离度量，衡量的是多维空间中两个点之间的<strong>绝对距离</strong> 。<br>$$<br>d &#x3D; \sqrt{ \sum\limits_{i&#x3D;1}^{n}(x_i-y_i)^2}<br>$$</p>
<h5 id="折叶损失"><a href="#折叶损失" class="headerlink" title="折叶损失"></a>折叶损失</h5><p>$$<br>max(0,-)<br>$$</p>
<p>在二分类情况下，公式如下：<br>$$<br>max(0,1-t·y)<br>$$<br>其中，y是预测值(-1到1之间)，t为目标值(1或 -1)。其含义为，y的值在 -1到1之间即可，并不鼓励 |y|&gt;1，即让某个样本能够正确分类就可以了，不鼓励分类器过度自信，当样本与分割线的距离超过1时并不会有任何奖励。目的在于使分类器更专注于整体的分类误差。</p>
<h6 id="变种"><a href="#变种" class="headerlink" title="变种"></a>变种</h6><p>在实际应用中，一方面，预测值y并不总是属于[-1,1]，也可能属于其他的取值范围；另一方面，很多时候我们希望训练的是两个元素之间的相似关系，而非样本的类别得分。所以下面的公式可能会更加常用： </p>
<p><em>L</em>( y, y′) &#x3D; max( 0, <a class="link"   target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=margin&spm=1001.2101.3001.7020" >margin <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> – (y–y′) )</p>
<p>​       &#x3D; max( 0, margin + (y′–y) )</p>
<p>​      &#x3D; max( 0, margin + y′ – y)</p>
<p>其中，y是正确预测的得分，y′是错误预测的得分，两者的差值可用来表示两种预测结果的相似关系，margin是一个由自己指定的安全系数。我们希望正确预测的得分高于错误预测的得分，且高出一个边界值 margin，换句话说，y越高越好，y′ 越低越好，(y–y′)越大越好，(y′–y)越小越好，但二者得分之差最多为margin就足够了，差距更大并不会有任何奖励。这样设计的目的在于，对单个样本正确分类只要有margin的把握就足够了，更大的把握则不必要，过分注重单个样本的分类效果反而有可能使整体的分类效果变坏。分类器应该更加专注于整体的分类误差。</p>
<h5 id="平方折叶损失"><a href="#平方折叶损失" class="headerlink" title="平方折叶损失"></a>平方折叶损失</h5><p>L2-SVM<br>$$<br>max(0,-)^2<br>$$</p>
<h5 id="二元支持向量机"><a href="#二元支持向量机" class="headerlink" title="二元支持向量机"></a>二元支持向量机</h5><p>$$<br>L_i &#x3D; Cmax(0,1-y_iw^Tx_i)+R(W)<br>$$</p>
<p>其中，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=C"
                      alt="[公式]"
                >是一个超参数，并且<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=y_i%5Cin%5C%7B-1,1%5C%7D"
                      alt="[公式]"
                >。可以认为本章节介绍的SVM公式包含了上述公式，上述公式是多类支持向量机公式只有两个分类类别的特例。也就是说，如果我们要分类的类别只有两个，那么公式就化为二元SVM公式。这个公式中的<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=C"
                      alt="[公式]"
                >和多类SVM公式中的<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=%5Clambda"
                      alt="[公式]"
                >都控制着同样的权衡，而且它们之间的关系是<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://www.zhihu.com/equation?tex=C%5Cpropto%5Cfrac%7B1%7D%7B%5Clambda%7D"
                      alt="[公式]"
                ></p>
<h5 id="不可导的损失函数"><a href="#不可导的损失函数" class="headerlink" title="不可导的损失函数"></a>不可导的损失函数</h5><p>由于max操作，损失函数中存在一些不可导点（kinks），这些点使得损失函数不可微，因为在这些不可导点，梯度是没有定义的。但是次梯度（subgradient）依然存在且常常被使用。</p>
<h5 id="中心差值公式"><a href="#中心差值公式" class="headerlink" title="中心差值公式"></a>中心差值公式</h5><p>$$<br>\lim_{h→0}\frac{f(x+h)-f(x-h)}{2h}<br>$$</p>
<h5 id="神经元饱和"><a href="#神经元饱和" class="headerlink" title="神经元饱和"></a>神经元饱和</h5><p>饱和神经网络是其中大多数隐藏节点的值都接近-1.0或+1.0且输出节点的值都接近0.0或1.0的网络。饱和不是一件好事。如果隐藏节点饱和，则意味着它们的激活前积和相对较大（通常大于4.0）或较小（通常小于-4.0）。</p>
<p>饱和节点会导致这样一种情况，即在训练过程中输入到隐藏权重的微小变化可能不会太大地改变乘积之和，然后在激活后，节点值仍将为-1.0或+1.0-换句话说，训练停滞不前或动作非常缓慢。此外，饱和模型经常过拟合-意味着该模型在训练数据上预测良好，而在新的，看不见的数据上预测较差。</p>
<h5 id="全连接层-1"><a href="#全连接层-1" class="headerlink" title="全连接层"></a>全连接层</h5><p>全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：</p>
<p>对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hw的全局卷积，hw分别为前层卷积结果的高和宽。</p>
<p>全连接的核心操作就是矩阵向量乘积 y &#x3D; Wx</p>
<p>本质就是由一个特征空间线性变换到另一个特征空间。目标空间的任一维——也就是隐层的一个 cell——都认为会受到源空间的每一维的影响。</p>
<h5 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h5><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pic1.zhimg.com/80/cf3fc543bf1dc81e2083530a4492b0ec_720w.png"
                      alt="img"
                ></p>
<p>过拟合（Overfitting）是网络对数据中的噪声有很强的拟合能力，而没有重视数据间（假设）的潜在基本关系。</p>
<p>举例来说，有20个神经元隐层的网络拟合了所有的训练数据，但是其代价是把决策边界变成了许多不相连的红绿区域。而有3个神经元的模型的表达能力只能用比较宽泛的方式去分类数据。它将数据看做是两个大块，并把个别在绿色区域内的红色点看做噪声。在实际中，这样可以在测试数据中获得更好的<strong>泛化（generalization）</strong>能力。</p>
<h5 id="泛化"><a href="#泛化" class="headerlink" title="泛化"></a>泛化</h5><p>泛化能力（generalization ability）是指机器学习算法对新鲜样本的适应能力。 学习的目的是学到隐含在数据背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。</p>
<h5 id="牛顿步长"><a href="#牛顿步长" class="headerlink" title="牛顿步长"></a>牛顿步长</h5><p>牛顿法法主要是为了解决非线性优化问题，其收敛速度比梯度下降速度更快。其需要解决的问题可以描述为：对于目标函数f(x)，在无约束条件的情况下求它的最小值。<br>$$<br>\min_{x\in R^n}f(x)<br>$$<br>其中x&#x3D;(x1,x2,..,xn)是n维空间的向量。我们在下面需要用到的泰勒公式先在这写出来。<br>$$<br>f(x)&#x3D;f(x_0)+f’(x_0)(x-x_0)+\frac{1}{2}f’’(x_0)(x-x_0)^2+R_n(x)<br>$$<br>牛顿法的主要思想是：在现有的极小值估计值的附近对f(x)做二阶泰勒展开，进而找到极小点的下一个估计值，反复迭代直到函数的一阶导数小于某个接近0的阀值。</p>
<p>1）n&#x3D;1，设x&#x3D;xt时，函数f(x)取得最小值，我们的目标就是希望能求得xt，现在我们设xk作为xt的估计值。我们在x&#x3D;xk处进行泰勒二阶展开，过程略。最终：<br>$$<br>x&#x3D;x_k-\frac{f’(x_k)}{f’’(x_k)}<br>$$<br>2）n&gt;1时，可以将x写成：x&#x3D;(x1,x2,…,xn)，同样，我们先对x进行泰勒展开：<br>$$<br>f(x)≈f(x_k)+f’(x_k)(x-x_k)+\frac{1}{2}f’’(x_k)(x-x_k)^2<br>$$<br>然后对f(x)求导，此处的求导比n&#x3D;1的情形要复杂一点，由于f(x)中的x是一个向量，f(x)对x求导意味着对x向量中的每个值求偏导。即，f(x)对x的一阶导数为一个向量，对x的二阶导数为一个n*n的矩阵。<br>$$<br>f’(x)&#x3D;(\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2},…,\frac{\partial f(x)}{\partial x_n})<br>$$</p>
<p>$$<br>f’’(x)&#x3D;[\frac{\partial ^2f(x)}{\part x_i\part x_j}]_{n×n}<br>$$</p>
<p>求导后得：<br>$$<br>f’(x)&#x3D;f’(x_k)+f’’(x_k)(x-x_k) \tag{1}<br>$$<br>为了方便表达，我们令：<br>$$<br>{\scr{g}} _k&#x3D;f’(x_k),,,,H_k&#x3D;f’’(x_k)<br>$$<br>此时的gk为一个向量，Hk为一个矩阵。</p>
<p>令（1）中的f(x)导数为0，得：<br>$$<br>f’(x_k)&#x3D;-f’’(x_k)(x-x_k)<br>$$<br>即：<br>$$<br>{\scr{g}} _k&#x3D;-H_k(x-x_k), ,,, x&#x3D;x_k-H_k^{-1}{\scr{g}}<em>k<br>$$<br>得到x的迭代值为：<br>$$<br>x</em>{k+1}&#x3D;x_k-H_k^{-1}{\scr{g}}_k<br>$$<br>牛顿法算法流程：</p>
<p>1）初始化x0，设置终止阀值a，令k&#x3D;0.</p>
<p>2）计算f(x)在x&#x3D;xk的梯度向量gk和海塞矩阵Hk-1的逆矩阵，其计算公式为：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://images0.cnblogs.com/blog2015/680781/201508/082141550183343.jpg"
                      alt="img"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://images0.cnblogs.com/blog2015/680781/201508/082142031439693.jpg"
                      alt="img"
                ></p>
<p>3）判断||gk||&lt;a，则停止计算，得到x&#x3D;xk即为所求。</p>
<p>4）更新x：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://images0.cnblogs.com/blog2015/680781/201508/082142437997206.jpg"
                      alt="img"
                >，转移至第2）步。</p>
<p>算法分析：</p>
<p>1）前提：</p>
<p>要使用该算法要满足两个条件：一是函数f(x)一阶，二阶可偏导；二是海塞矩阵要求正定。若要使得最终结果能收敛到最小值，则f(x)需要为凸函数。</p>
<p>当f(x)满足：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://images0.cnblogs.com/blog2015/680781/201508/082143466598401.jpg"
                      alt="img"
                >时，海塞矩阵为对称矩阵，对所有特征值大于0的对称矩阵称为正定矩阵。正定矩阵一定是非奇异矩阵。</p>
<p>附：若A为奇异矩阵，则有：|A|&#x3D;0.</p>
<p>2）优点：</p>
<p>​     它比传统的梯度下降算法收敛速度明显要快，另外，当f(x)是二次函数时，仅需一次迭代就能直接收敛到最小值。因为f(x)为二次函数时，在进行泰勒展开式，并没有高阶导数的损失，而在后面的每次计算都是等号运算，因而最后得到结果就是函数f(x)最小值对应的x。而对于非二次函数，若函数的二次形态较强，或迭代点已进入极小点的领域内，则其收敛速度也会很快。</p>
<p>3）缺点：</p>
<p>A）计算复杂度问题。</p>
<p>　　在上面的这个算法中存在一个问题，即海塞矩阵的计算问题，此问题需要对f(x)求二阶偏导数，计算开销很大；另一方面，海塞矩阵的大小是n的平方，当n增大时，存储和计算的量是平方的速度增加。</p>
<p>　　针对计算复杂度问题，于是有了拟牛顿法。</p>
<p>B）收敛性问题。</p>
<p>　　对于非二次函数，也可能会出现f(xk+1)&gt;f(xk)的情况，这时，牛顿法不能收敛，从而导致计算失败。对于这种情况，可以使用阻尼牛顿法来解决。</p>
<p>　　阻尼牛顿法最核心的一点在于可以修改每次迭代的步长，通过沿着牛顿法确定的方向一维搜索最优的步长，最终选择使得函数值最小的步长。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://images0.cnblogs.com/blog2015/680781/201508/082145081436842.jpg"
                      alt="img"
                ></p>
<p>　　关于步长的搜索，主要有精确搜索和非精确搜索，上面的方法是一种精确地搜索方法，在实际中还有Wolfe型搜索，Armijo搜索以及满足Goldstein条件的非精度搜索。具体的搜索方法大家可自行研究。</p>
<h5 id="鲁棒性"><a href="#鲁棒性" class="headerlink" title="鲁棒性"></a>鲁棒性</h5><p>鲁棒是Robust的音译，也就是健壮和强壮的意思。它也是在异常和危险情况下系统生存的能力。比如说，计算机软件在输入错误、磁盘故障、网络过载或有意攻击情况下，能否不死机、不崩溃，就是该软件的鲁棒性。</p>
<h5 id="海森矩阵（二阶偏导矩阵）"><a href="#海森矩阵（二阶偏导矩阵）" class="headerlink" title="海森矩阵（二阶偏导矩阵）"></a>海森矩阵（二阶偏导矩阵）</h5><p>海森矩阵（Hessian Matrix），又译作黑塞矩阵、海瑟矩阵、 海塞矩阵等，是一个多元函数的二阶偏导数构成的方阵，描述 了函数的局部曲率。黑塞矩阵最早于19世纪由德国数学家 Ludwig Otto Hesse提出，并以其名字命名。海森矩阵常用于 解决优化问题，利用黑塞矩阵可判定多元函数的极值问题。</p>
<p>$$<br>A&#x3D;<br>\begin{bmatrix}<br>\frac{\part ^2f}{\part x_1^2} &amp; \frac{\part ^2f}{\part x_1 \part x_2} &amp;… &amp;\frac{\part ^2f}{\part x_1 \part x_n} \<br>\frac{\part ^2f}{\part x_2 \part x_1} &amp; \frac{\part ^2f}{\part x_2^2} &amp;… &amp;\frac{\part ^2f}{\part x_2 \part x_n} \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>\frac{\part ^2f}{\part x_n \part x_1} &amp; \frac{\part ^2f}{\part x_n \part x_2} &amp;… &amp;\frac{\part ^2f}{\part x_n^2}<br>\end{bmatrix}<br>$$</p>
<h5 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h5><p><a class="link"   target="_blank" rel="noopener" href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" >Batch Normalization <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>归一化的反向传播：<a class="link"   target="_blank" rel="noopener" href="https://kevinzakka.github.io/2016/09/14/batch_normalization/" >https://kevinzakka.github.io/2016/09/14/batch_normalization/ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>使用浅层模型时，随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。对深层神经网络来说，随着网络训练的进行，前一层参数的调整使得后一层输入数据的分布发生变化，各层在训练的过程中就需要不断的改变以适应学习这种新的数据分布。所以即使输入数据已做标准化，训练中模型参数的更新依然很容易导致后面层输入数据分布的变化，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。最终造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。如果训练过程中，训练数据的分布一直在发生变化，那么将不仅会增大训练的复杂度，影响网络的训练速度而且增加了过拟合的风险。</p>
<p>在模型训练时，在应用激活函数之前，先对一个层的输出进行归一化，将所有批数据强制在统一的数据分布下，然后再将其输入到下一层，使整个神经网络在各层的中间输出的数值更稳定。从而使深层神经网络更容易收敛而且降低模型过拟合的风险。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG"
                      alt="img"
                ></p>
<h5 id="硬注意力、软注意力"><a href="#硬注意力、软注意力" class="headerlink" title="硬注意力、软注意力"></a>硬注意力、软注意力</h5><p>软性注意力（Soft Attention）机制是指在选择信息的时候，不是从N个信息中只选择1个，而是计算N个输入信息的加权平均，再输入到神经网络中计算。</p>
<p>硬性注意力（Hard Attention）就是指选择输入序列某一个位置上的信息，比如随机选择一个信息或者选择概率最高的信息。但一般还是用软性注意力机制来处理神经网络的问题。</p>
<h3 id="涉及代码"><a href="#涉及代码" class="headerlink" title="涉及代码"></a>涉及代码</h3><h5 id="X-shape-0-1"><a href="#X-shape-0-1" class="headerlink" title="X.shape[0], -1"></a>X.shape[0], -1</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.reshape(X.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure></div>

<p>将一个维度为(a, b, c, d)的矩阵转换为一个维度为(a, b * c * d)的矩阵</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X.shape</span><br><span class="line">(<span class="number">209</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X.shape[<span class="number">0</span>]</span><br><span class="line"><span class="number">209</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X.reshape(X.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">(<span class="number">209</span>, <span class="number">64</span>*<span class="number">64</span>*<span class="number">3</span>)</span><br></pre></td></tr></table></figure></div>



<p>数据增广</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apply(img, torchvision.transforms.RandomHorizontalFlip())  <span class="comment"># 左右随机翻转</span></span><br><span class="line">apply(img, torchvision.transforms.RandomVerticalFlip())  <span class="comment"># 上下随机翻转</span></span><br><span class="line"></span><br><span class="line">color_aug = torchvision.transforms.ColorJitter(</span><br><span class="line">	brightness=<span class="number">0.5</span>, contrast=<span class="number">0.5</span>, saturation-<span class="number">0.5</span>, hue=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">augs = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug</span><br><span class="line">])</span><br><span class="line">apply(img, augs)</span><br></pre></td></tr></table></figure></div>





<h3 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h3><p>KNN</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/zhyh1435589631/article/details/54236643" >https://blog.csdn.net/zhyh1435589631/article/details/54236643 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>SVM</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37041483/article/details/99082602" >https://blog.csdn.net/qq_37041483/article/details/99082602 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>整体(作业1)</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/30748903" >https://zhuanlan.zhihu.com/p/30748903 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>fc_net.py代码</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/xieyi4650/article/details/53932114" >https://blog.csdn.net/xieyi4650/article/details/53932114 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>反向传播</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" >https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>作业代码来源(最新版本)</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://github.com/qxdn/cs231n-assignment" >https://github.com/qxdn/cs231n-assignment <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>

            </div>

            

            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2022/08/07/2022-08-07-cs231n-assignment/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">cs231n assignment</span>
                                    <span class="post-nav-item">上一篇</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2022/07/30/2022-07-30-CS231N/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">CS231N</span>
                                    <span class="post-nav-item">下一篇</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">cs231n笔记</div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#cs231n"><span class="nav-text">cs231n</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Nearest-Neighbor%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-text">Nearest Neighbor分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB"><span class="nav-text">线性分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E5%8C%96Optimization"><span class="nav-text">最优化Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5"><span class="nav-text">策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-text">梯度计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-text">线性分类器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-text">常用激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-text">神经网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97%E4%B8%BE%E4%BE%8B"><span class="nav-text">前向传播计算举例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%82%E7%9A%84%E6%95%B0%E9%87%8F%E5%92%8C%E5%B0%BA%E5%AF%B8"><span class="nav-text">层的数量和尺寸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">权重初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-1"><span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%9F%A5"><span class="nav-text">梯度检查</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%88%E7%90%86%E5%8C%96%E6%A3%80%E6%9F%A5"><span class="nav-text">合理化检查</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B"><span class="nav-text">检查学习过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"><span class="nav-text">参数更新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%90%E5%8F%82%E6%95%B0%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%96%B9%E6%B3%95"><span class="nav-text">逐参数适应学习率方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="nav-text">超参数调优</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7"><span class="nav-text">评价</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90"><span class="nav-text">模型集成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNNs"><span class="nav-text">卷积神经网络CNNs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="nav-text">结构概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%87%E8%81%9A%E5%B1%82%EF%BC%88%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%89"><span class="nav-text">汇聚层（池化层）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%B1%82"><span class="nav-text">归一层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="nav-text">全连接层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E6%9E%84"><span class="nav-text">结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNNs"><span class="nav-text">循环神经网络RNNs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-text">RNN网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%86%E5%88%AB%E5%92%8C%E5%88%86%E5%89%B2"><span class="nav-text">识别和分割</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E7%90%86%E8%A7%A3"><span class="nav-text">可视化理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">无监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0"><span class="nav-text">深度增强学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96-1"><span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B6%89%E5%8F%8A%E8%AF%8D%E6%B1%87"><span class="nav-text">涉及词汇</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B6%89%E5%8F%8A%E4%BB%A3%E7%A0%81"><span class="nav-text">涉及代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%9C%E4%B8%9A"><span class="nav-text">作业</span></a></li></ol></li></ol></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2022</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">staaar</a>
        </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a> 驱动</span>
                <br>
            <span class="theme-version-container">主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.1.0</a>
        </div>
        
        
        
        
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/navbarShrink.js"></script>

<script src="/js/tools/scrollTopBottom.js"></script>

<script src="/js/tools/lightDarkSwitch.js"></script>





    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>






  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts pjax">
    
        
<script src="/js/tools/tocToggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




</body>
</html>
